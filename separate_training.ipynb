{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e8d51fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax backend cpu\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time, os, json\n",
    "import pandas as pd\n",
    "from scipy import stats \n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import jax\n",
    "from jax import random\n",
    "from jax.config import config \n",
    "import jax.numpy as np\n",
    "from jax import vmap\n",
    "import pdb\n",
    "import optax\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "from jax.lib import xla_bridge\n",
    "print(\"jax backend {}\".format(xla_bridge.get_backend().platform))\n",
    "#config.update('jax_debug_nans', True)\n",
    "from SSN_classes_jax import SSN2DTopoV1_AMPAGABA_ONOFF\n",
    "from util import GaborFilter, BW_Grating, find_A, create_gabor_filters, create_gratings, param_ratios, plot_results\n",
    "from training_script import train_SSN_vmap\n",
    "\n",
    "#initialize key\n",
    "key = random.PRNGKey(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "990eda40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gabor parameters \n",
    "sigma_g= 0.5\n",
    "k = np.pi/(6*sigma_g)\n",
    "\n",
    "#Stimuli parameters\n",
    "ref_ori = 55\n",
    "offset = 5\n",
    "\n",
    "#Assemble parameters in dictionary\n",
    "general_pars = dict(k=k , edge_deg=3.2,  degree_per_pixel=0.05)\n",
    "stimuli_pars = dict(outer_radius=3, inner_radius=2.5, grating_contrast=0.8, std = 0, jitter_val = 5)\n",
    "stimuli_pars.update(general_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "948b8352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network parameters\n",
    "class ssn_pars():\n",
    "    n = 2\n",
    "    k = 0.04\n",
    "    tauE = 30 # in ms\n",
    "    tauI = 10 # in ms~\n",
    "    psi = 0.774\n",
    "    A=None\n",
    "    tau_s = np.array([5, 7, 100]) #in ms, AMPA, GABA, NMDA current decay time constants\n",
    "    \n",
    "\n",
    "#Grid parameters\n",
    "class grid_pars():\n",
    "    gridsize_Nx = 9 # grid-points across each edge # gives rise to dx = 0.8 mm\n",
    "    gridsize_deg = 2 * 1.6 # edge length in degrees\n",
    "    magnif_factor = 2  # mm/deg\n",
    "    hyper_col = 0.8 # mm   \n",
    "    sigma_RF = 0.4 # deg (visual angle)\n",
    "\n",
    "# Caleb's params for the full (with local) model:\n",
    "Js0 = [1.82650658, 0.68194475, 2.06815311, 0.5106321]\n",
    "gE, gI = 0.57328625, 0.26144141\n",
    "\n",
    "sigEE, sigIE = 0.2, 0.40\n",
    "sigEI, sigII = .09, .09\n",
    "#sigEI, sigII = .2, .2\n",
    "conn_pars = dict(\n",
    "    PERIODIC = False,\n",
    "    p_local = [.4, 0.7], # [p_local_EE, p_local_IE],\n",
    "    sigma_oris = 1000) # sigma_oris\n",
    "\n",
    "\n",
    "make_J2x2 = lambda Jee, Jei, Jie, Jii: np.array([[Jee, -Jei], [Jie,  -Jii]]) * np.pi * ssn_pars.psi\n",
    "J_2x2 = make_J2x2(*Js0)\n",
    "s_2x2 = np.array([[sigEE, sigEI],[sigIE, sigII]])\n",
    "\n",
    "#Parameters exclusive to Gabor filters\n",
    "filter_pars = dict(sigma_g = sigma_g, conv_factor = grid_pars.magnif_factor)\n",
    "filter_pars.update(general_pars) \n",
    "\n",
    "#Positive reparameterization\n",
    "signs=np.array([[1, -1], [1, -1]])\n",
    "logJ_2x2 =np.log(J_2x2*signs)\n",
    "logs_2x2 = np.log(s_2x2)\n",
    "\n",
    "#Excitatory and inhibitory constants for extra synaptic GABA\n",
    "c_E = 5.0\n",
    "c_I = 5.0\n",
    "\n",
    "#Sigmoid parameters\n",
    "N_neurons = 25\n",
    "\n",
    "#key, _ = random.split(key)\n",
    "#w_sig = random.normal(key, shape = (N_neurons,)) / np.sqrt(N_neurons)\n",
    "w_sig = numpy.random.normal(size=(N_neurons,)) / np.sqrt(N_neurons)\n",
    "#w_sig = np.zeros((N_neurons))\n",
    "b_sig = 0.0\n",
    "\n",
    "\n",
    "#New parameter definition\n",
    "ssn_layer_pars = dict(logJ_2x2 = logJ_2x2, logs_2x2 = logs_2x2,  c_E = c_E, c_I = c_I)\n",
    "readout_pars = dict(w_sig = w_sig, b_sig=b_sig)\n",
    "\n",
    "#Convergence parameters\n",
    "conv_pars=dict(dt = 1, xtol = 1e-5, Tmax = 1000, verbose=False, silent=True)\n",
    "np.linalg.norm(readout_pars['w_sig'])\n",
    "\n",
    "loss_pars = dict(lambda_1=1, lambda_2=1, lambda_w= 1.5, lambda_b=1)\n",
    "\n",
    "readout_pars['w_sig'] = np.asarray([ 0.02254014,  0.23776476, -0.22387406, -0.17959306,\n",
    "              0.03952925, -0.16213389,  0.2610552 ,  0.0174748 ,\n",
    "              0.09076759,  0.06380674,  0.01412422, -0.06971945,\n",
    "             -0.04105902,  0.31636822,  0.0152253 ,  0.0937705 ,\n",
    "             -0.29950964, -0.04642078, -0.29968715, -0.11778114,\n",
    "              0.25034818,  0.05684045,  0.28512332, -0.0812244 ,\n",
    "             -0.10014342])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "269f7999",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conn_pars():\n",
    "    PERIODIC = False\n",
    "    p_local = [0.4, 0.7]\n",
    "    sigma_oris = 1000\n",
    "    \n",
    "class filter_pars():\n",
    "    sigma_g = numpy.array(0.5)\n",
    "    conv_factor = numpy.array(2)\n",
    "    k = numpy.array(1.0471975511965976)\n",
    "    edge_deg = numpy.array( 3.2)\n",
    "    degree_per_pixel = numpy.array(0.05)\n",
    "    \n",
    "class conv_pars:\n",
    "    dt = 1\n",
    "    xtol = 1e-05\n",
    "    Tmax = 1000\n",
    "    verbose = False\n",
    "    silent = True\n",
    "\n",
    "class loss_pars:\n",
    "    lambda_1 = 1\n",
    "    lambda_2 = 1\n",
    "    lambda_w = 1\n",
    "    lambda_b = 1\n",
    "    \n",
    "    \n",
    "ssn=SSN2DTopoV1_AMPAGABA_ONOFF(ssn_pars=ssn_pars, grid_pars=grid_pars, conn_pars=conn_pars, filter_pars=filter_pars, J_2x2=J_2x2, s_2x2=s_2x2)\n",
    "ssn_pars.A=ssn.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c9dcfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of epochs\n",
    "epochs = 100\n",
    "num_epochs_to_save = 21\n",
    "epochs_to_save = np.linspace(1 ,epochs, num_epochs_to_save).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b16ee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(training_losses):\n",
    "    plt.plot(training_losses.T, label = ['Binary cross entropy', 'Avg_dx', 'R_max', 'w', 'b', 'Total'] )\n",
    "    plt.legend()\n",
    "    plt.title('Training losses')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_results(results_file, title=None):\n",
    "    \n",
    "    results = pd.read_csv(results_file, header = 0)\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12,8))\n",
    "\n",
    "    results.plot(x='epoch', y=[\"J_EE\", \"J_EI\", \"J_IE\", \"J_II\"], ax=axes[0,0])\n",
    "    results.plot(x='epoch', y=[\"s_EE\", \"s_EI\", \"s_IE\", \"s_II\"], ax = axes[0,1])\n",
    "    results.plot(x='epoch', y=[\"c_E\", \"c_I\"], ax = axes[1,0])\n",
    "    results.plot(x='epoch', y = ['val_accuracy', 'ber_acc'], ax = axes[1,1])\n",
    " \n",
    "    \n",
    "    if title:\n",
    "        fig.suptitle(title)\n",
    "    fig.show()\n",
    "    \n",
    "def create_data(stimuli_pars, number=100, offset = 5, ref_ori=55):\n",
    "    \n",
    "    '''\n",
    "    Create data for given jitter and noise value for testing (not dataloader)\n",
    "    '''\n",
    "    data = create_gratings(ref_ori=ref_ori, number=number, offset=offset, **stimuli_pars)\n",
    "    train_data = next(iter(DataLoader(data, batch_size=len(data), shuffle=False)))\n",
    "    train_data['ref'] = train_data['ref'].numpy()\n",
    "    train_data['target'] = train_data['target'].numpy()\n",
    "    train_data['label'] = train_data['label'].numpy()\n",
    "    \n",
    "    return train_data\n",
    "\n",
    "\n",
    "def save_params_dict(ssn_layer_pars, readout_pars , true_acc, ber_acc, epoch ):\n",
    "    \n",
    "    J_2x2, s_2x2 = exponentiate(ssn_layer_pars)\n",
    "     \n",
    "    save_params= dict(val_accuracy= true_acc, \n",
    "                      ber_acc = ber_acc,\n",
    "                J_EE= J_2x2[0,0], J_EI = J_2x2[0,1], \n",
    "                              J_IE = J_2x2[1,0], J_II = J_2x2[1,1], \n",
    "                s_EE= s_2x2[0,0], s_EI = s_2x2[0,1], \n",
    "                              s_IE = s_2x2[1,0], s_II = s_2x2[1,1],\n",
    "                c_E = ssn_layer_pars['c_E'], c_I = ssn_layer_pars['c_I'], \n",
    "                 epoch = epoch, w_sig = readout_pars['w_sig'], b_sig=readout_pars['b_sig'])\n",
    "    \n",
    "    return save_params\n",
    "\n",
    "def constant_to_vec(c_E, c_I):\n",
    "    \n",
    "    matrix_E = np.zeros((9,9))\n",
    "    matrix_E = matrix_E.at[2:7, 2:7].set(c_E)\n",
    "    vec_E = np.ravel(matrix_E)\n",
    "    \n",
    "    matrix_I = np.zeros((9,9))\n",
    "    matrix_I = matrix_I.at[2:7, 2:7].set(c_I)\n",
    "    vec_I = np.ravel(matrix_I)\n",
    "    \n",
    "    constant_vec = np.hstack((vec_E, vec_E, vec_I, vec_I))\n",
    "    return constant_vec\n",
    "\n",
    "def sigmoid(x, epsilon = 0.001):\n",
    "    '''\n",
    "    Introduction of epsilon stops asymptote from reaching 1 (avoids NaN)\n",
    "    '''\n",
    "    sig = 1/(1+np.exp(x))\n",
    "    \n",
    "    return (1 - 2*epsilon)*sig + epsilon\n",
    "\n",
    "def binary_loss(n, x):\n",
    "    return - (n*np.log(x) + (1-n)*np.log(1-x))\n",
    "\n",
    "def exponentiate(opt_pars):\n",
    "    signs=np.array([[1, -1], [1, -1]]) \n",
    "    \n",
    "    J_2x2 =np.exp(opt_pars['logJ_2x2'])*signs\n",
    "    s_2x2 = np.exp(opt_pars['logs_2x2'])\n",
    "    \n",
    "    return J_2x2, s_2x2\n",
    "\n",
    "def our_max(x, beta=1):\n",
    "    max_val = np.log(np.sum(np.exp(x*beta)))/beta\n",
    "    return max_val\n",
    "\n",
    "def obtain_fixed_point(ssn, ssn_input, conv_pars,  Rmax_E = 50, Rmax_I = 100):\n",
    "    \n",
    "    r_init = np.zeros(ssn_input.shape[0])\n",
    "    \n",
    "    dt = conv_pars.dt\n",
    "    xtol = conv_pars.xtol\n",
    "    Tmax = conv_pars.Tmax\n",
    "    verbose = conv_pars.verbose\n",
    "    silent = conv_pars.silent\n",
    "    \n",
    "    #Find fixed point  \n",
    "    \n",
    "    fp, _, avg_dx = ssn.fixed_point_r(ssn_input, r_init=r_init, dt=dt, xtol=xtol, Tmax=Tmax, verbose = verbose, silent=silent)\n",
    "    avg_dx = np.maximum(0, (avg_dx -1))\n",
    "    \n",
    "    #Apply bounding box to data\n",
    "    x_box = ssn.apply_bounding_box(fp, size=3.2)\n",
    "    r_max = np.maximum(0, (our_max(fp[:ssn.Ne])/Rmax_E - 1)) + np.maximum(0, (our_max(fp[ssn.Ne:-1])/Rmax_I - 1))\n",
    "    \n",
    "    return x_box, r_max, avg_dx\n",
    "\n",
    "@partial(jax.jit, static_argnums=[2, 3, 4, 6, 7, 8, 9, 10])\n",
    "def model(ssn_layer_pars, readout_pars, ssn_pars, grid_pars, conn_pars, train_data, filter_pars,  conv_pars, loss_pars, bernoulli, sig_noise, Rmax_E = 50, Rmax_I = 100):\n",
    "    \n",
    "    J_2x2, s_2x2 = exponentiate(ssn_layer_pars)\n",
    "    \n",
    "    #Initialise network\n",
    "    ssn=SSN2DTopoV1_AMPAGABA_ONOFF(ssn_pars=ssn_pars, grid_pars=grid_pars, conn_pars=conn_pars, filter_pars=filter_pars, J_2x2=J_2x2, s_2x2=s_2x2)\n",
    "    \n",
    "    #Create vector using extrasynaptic constants\n",
    "    constant_vector = constant_to_vec(ssn_layer_pars['c_E'], ssn_layer_pars['c_I'])\n",
    "    \n",
    "    #Apply Gabor filters to stimuli\n",
    "    output_ref=np.matmul(ssn.gabor_filters, train_data['ref']) + constant_vector\n",
    "    output_target=np.matmul(ssn.gabor_filters, train_data['target']) + constant_vector\n",
    "    \n",
    "    #Rectify output\n",
    "    SSN_input_ref=np.maximum(0, output_ref)\n",
    "    SSN_input_target=np.maximum(0, output_target)\n",
    "\n",
    "    #Find the fixed point \n",
    "    x_ref, r_max_ref, avg_dx_ref = obtain_fixed_point(ssn, SSN_input_ref, conv_pars)\n",
    "    x_target, r_max_target, avg_dx_target = obtain_fixed_point(ssn, SSN_input_target, conv_pars)\n",
    "    \n",
    "    #Add additional noise before sigmoid layer\n",
    "    if sig_noise:\n",
    "        delta_x = x_ref.ravel() - x_target.ravel() + sig_noise*numpy.random.normal(size=((x_target.ravel()).shape))\n",
    "    else:\n",
    "        delta_x = x_ref.ravel() - x_target.ravel() \n",
    "    \n",
    "    #Apply sigmoid function \n",
    "    x = sigmoid(np.dot(readout_pars['w_sig'], (delta_x)) + readout_pars['b_sig'])\n",
    "    \n",
    "    #Calculate losses\n",
    "    loss_binary=binary_loss(train_data['label'], x)\n",
    "    loss_avg_dx = loss_pars.lambda_1*(avg_dx_ref + avg_dx_target)/2\n",
    "    loss_r_max =  loss_pars.lambda_2*(r_max_ref + r_max_target)/2\n",
    "    loss_w = loss_pars.lambda_w*(np.linalg.norm(readout_pars['w_sig'])**2)\n",
    "    loss_b = loss_pars.lambda_b*(readout_pars['b_sig']**2)\n",
    "    \n",
    "    #Combine loss with maximum rate and dx value\n",
    "    loss = loss_binary + loss_avg_dx +  loss_r_max  + loss_b  #+ loss_w\n",
    "    \n",
    "    #Find predicted label using Bernoulli distribution\n",
    "    pred_label = np.round(x) \n",
    "    if bernoulli==True:\n",
    "        key_int = numpy.random.randint(low = 0, high =  10000)\n",
    "        key = random.PRNGKey(key_int)\n",
    "        pred_label_b = np.sum(jax.random.bernoulli(key, p=x, shape=None))\n",
    "        pred_label = [pred_label, pred_label_b]\n",
    "        \n",
    "    \n",
    "    all_losses = np.vstack((loss_binary, loss_avg_dx, loss_r_max, loss_w, loss_b, loss))\n",
    "    \n",
    "    return loss, all_losses, pred_label\n",
    "\n",
    "def loss(ssn_layer_pars, readout_pars, ssn_pars, grid_pars, conn_pars, train_data, filter_pars,  conv_pars, loss_pars, bernoulli, sig_noise):\n",
    "    \n",
    "    '''\n",
    "    Calculate parallelized loss for batch of data through vmap.\n",
    "    Output:\n",
    "        mean loss of all the input images\n",
    "    '''\n",
    "    \n",
    "    vmap_model = vmap(model, in_axes = ({'c_E':None, 'c_I': None, 'logJ_2x2': None, 'logs_2x2': None}, {'b_sig': None, 'w_sig': None}, None, None, None,  {'ref':0, 'target':0, 'label':0}, None, None, None, None, None) )                   \n",
    "    total_loss, all_losses, _ = vmap_model(ssn_layer_pars, readout_pars, ssn_pars, grid_pars, conn_pars, train_data, filter_pars,  conv_pars, loss_pars, bernoulli, sig_noise)\n",
    "    loss= np.sum(total_loss)\n",
    "    all_losses = np.mean(all_losses, axis = 0)\n",
    "    \n",
    "    return loss, all_losses\n",
    "\n",
    "\n",
    "def vmap_eval(ssn_layer_pars, readout_pars, ssn_pars, grid_pars, conn_pars, test_data, filter_pars,  conv_pars, loss_pars, bernoulli, sig_noise):\n",
    "    \n",
    "    eval_vmap = vmap(model, in_axes = ({'c_E':None, 'c_I': None, 'logJ_2x2': None, 'logs_2x2': None}, {'b_sig': None, 'w_sig': None}, None, None, None,  {'ref':0, 'target':0, 'label':0}, None, None, None, None, None) )\n",
    "    losses, _, pred_labels = eval_vmap(ssn_layer_pars, readout_pars, ssn_pars, grid_pars, conn_pars, test_data, filter_pars,  conv_pars, loss_pars, bernoulli, sig_noise)\n",
    "    \n",
    "    true_acc = np.sum(test_data['label'] == pred_labels[0])/len(test_data['label']) \n",
    "    ber_acc = np.sum(test_data['label'] == pred_labels[1])/len(test_data['label']) \n",
    "    \n",
    "    vmap_loss= np.mean(losses)\n",
    "    \n",
    "    return vmap_loss, true_acc, ber_acc\n",
    "\n",
    "\n",
    "def separate_train_SSN_vmap(ssn_layer_pars, readout_pars, ssn_pars, grid_pars, conn_pars, stimuli_pars, filter_pars, conv_pars, loss_pars, epochs_to_save, opt, results_filename = None, batch_size=20, test_size = 100, ref_ori = 55, offset = 5, epochs=1, eta=10e-4, early_stop=0.7, bernoulli = False, sig_noise = None):\n",
    "    \n",
    "    #Initialize loss\n",
    "    val_loss_per_epoch = []\n",
    "    training_losses=[]\n",
    "        \n",
    "    #Initialise optimizer\n",
    "    optimizer = optax.adam(eta)\n",
    "    if opt=='readout':\n",
    "        opt_state = optimizer.init(readout_pars)\n",
    "    if opt=='ssn':\n",
    "        opt_state = optimizer.init(ssn_layer_pars)\n",
    "    \n",
    "    #Define test data - no need to iterate\n",
    "    test_data = create_data(stimuli_pars, number = test_size, offset = offset, ref_ori = ref_ori)\n",
    "    val_loss, true_acc, ber_acc = vmap_eval(ssn_layer_pars, readout_pars, ssn_pars, grid_pars, conn_pars, test_data, filter_pars, conv_pars, loss_pars, bernoulli, sig_noise)\n",
    "    print('Before training  -- loss: {}, true accuracy: {}, Bernoulli accuracy: {} '.format(val_loss, true_acc, ber_acc))\n",
    "    val_loss_per_epoch.append(val_loss)\n",
    "    \n",
    "    #Save initial parameters\n",
    "    save_params = save_params_dict(ssn_layer_pars, readout_pars, true_acc, ber_acc, epoch=0)\n",
    "    \n",
    "    #Initialise csv file\n",
    "    if results_filename:\n",
    "        results_handle = open(results_filename, 'w')\n",
    "        results_writer = csv.DictWriter(results_handle, fieldnames=save_params.keys())\n",
    "        results_writer.writeheader()\n",
    "        results_writer.writerow(save_params)\n",
    "        print('Saving results to csv ', results_filename)\n",
    "    else:\n",
    "        print('#### NOT SAVING!!#####')\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0 \n",
    "           \n",
    "        #load next batch of data and convert\n",
    "        train_data = create_data(stimuli_pars, number = batch_size, offset = offset, ref_ori = ref_ori)\n",
    "\n",
    "        #Compute loss and gradient + apply SGD\n",
    "        if opt=='readout':\n",
    "            epoch_loss, grad =jax.value_and_grad(loss, argnums=1, has_aux=True)(ssn_layer_pars, readout_pars, ssn_pars, grid_pars, conn_pars, train_data, filter_pars,  conv_pars, loss_pars, bernoulli, sig_noise)\n",
    "            updates, opt_state = optimizer.update(grad, opt_state)\n",
    "            readout_pars = optax.apply_updates(readout_pars, updates)\n",
    "            \n",
    "        if opt=='ssn':\n",
    "            epoch_loss, grad =jax.value_and_grad(loss, argnums=0, has_aux = True)(ssn_layer_pars, readout_pars, ssn_pars, grid_pars, conn_pars, train_data, filter_pars,  conv_pars, loss_pars, bernoulli, sig_noise)\n",
    "            updates, opt_state = optimizer.update(grad, opt_state)\n",
    "            ssn_layer_pars = optax.apply_updates(ssn_layer_pars, updates)\n",
    "        \n",
    "        #Stack all training losses\n",
    "        if epoch==1:\n",
    "            all_losses = epoch_loss[1]\n",
    "        else:\n",
    "            all_losses = np.hstack((all_losses, epoch_loss[1]))\n",
    "        \n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        #Save the parameters given a number of epochs\n",
    "        if epoch in epochs_to_save:\n",
    "            \n",
    "            #Evaluate model \n",
    "            test_data = create_data(stimuli_pars, number = test_size, offset = offset, ref_ori = ref_ori)\n",
    "            val_loss, true_acc, ber_acc= vmap_eval(ssn_layer_pars, readout_pars, ssn_pars, grid_pars, conn_pars, test_data, filter_pars,  conv_pars, loss_pars, bernoulli, sig_noise)\n",
    "            print('Training loss: {} ¦ Validation -- loss: {}, true accuracy: {}, Bernoulli accuracy: {},  at epoch {}, (time {})'.format(epoch_loss[0], val_loss, true_acc, ber_acc, epoch, epoch_time))\n",
    "            val_loss_per_epoch.append(val_loss)\n",
    "            \n",
    "            #Create dictionary of parameters to save\n",
    "            save_params = save_params_dict(ssn_layer_pars, readout_pars, true_acc, ber_acc, epoch)\n",
    "            \n",
    "            #Write results in csv file\n",
    "            if results_filename:\n",
    "                results_writer.writerow(save_params)\n",
    "                \n",
    "           #Early stop to train readout layer \n",
    "            if true_acc>= early_stop and opt=='readout':\n",
    "                print('Early stop: {} accuracy achieved'.format(early_stop))\n",
    "                break\n",
    "    \n",
    "    return ssn_layer_pars, readout_pars, val_loss_per_epoch, all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09ebced9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training  -- loss: 0.768050491809845, true accuracy: 0.550000011920929, Bernoulli accuracy: 0.44999998807907104 \n",
      "#### NOT SAVING!!#####\n",
      "Training loss: 38.524574279785156 ¦ Validation -- loss: 0.7462537288665771, true accuracy: 0.3799999952316284, Bernoulli accuracy: 0.3799999952316284,  at epoch 1, (time 16.575222492218018)\n",
      "Training loss: 44.280860900878906 ¦ Validation -- loss: 0.7995284795761108, true accuracy: 0.4000000059604645, Bernoulli accuracy: 0.4000000059604645,  at epoch 5, (time 1.4103031158447266)\n",
      "Training loss: 40.9765625 ¦ Validation -- loss: 0.7331077456474304, true accuracy: 0.5199999809265137, Bernoulli accuracy: 0.5199999809265137,  at epoch 10, (time 1.3933753967285156)\n",
      "Training loss: 42.042991638183594 ¦ Validation -- loss: 0.7476299405097961, true accuracy: 0.44999998807907104, Bernoulli accuracy: 0.44999998807907104,  at epoch 15, (time 1.4030799865722656)\n",
      "Training loss: 38.10869598388672 ¦ Validation -- loss: 0.734375536441803, true accuracy: 0.44999998807907104, Bernoulli accuracy: 0.44999998807907104,  at epoch 20, (time 1.454209327697754)\n",
      "Training loss: 36.123252868652344 ¦ Validation -- loss: 0.7444305419921875, true accuracy: 0.5299999713897705, Bernoulli accuracy: 0.4699999988079071,  at epoch 25, (time 1.4459805488586426)\n",
      "Training loss: 35.05805206298828 ¦ Validation -- loss: 0.7094395160675049, true accuracy: 0.4099999964237213, Bernoulli accuracy: 0.5899999737739563,  at epoch 30, (time 1.4266209602355957)\n",
      "Training loss: 34.372779846191406 ¦ Validation -- loss: 0.7344444990158081, true accuracy: 0.5199999809265137, Bernoulli accuracy: 0.5199999809265137,  at epoch 35, (time 1.393491268157959)\n",
      "Training loss: 34.14383316040039 ¦ Validation -- loss: 0.6801366806030273, true accuracy: 0.550000011920929, Bernoulli accuracy: 0.44999998807907104,  at epoch 40, (time 1.3916218280792236)\n",
      "Training loss: 37.13140106201172 ¦ Validation -- loss: 0.6505297422409058, true accuracy: 0.5299999713897705, Bernoulli accuracy: 0.4699999988079071,  at epoch 45, (time 1.382786750793457)\n",
      "Training loss: 33.30445861816406 ¦ Validation -- loss: 0.6920828223228455, true accuracy: 0.49000000953674316, Bernoulli accuracy: 0.5099999904632568,  at epoch 50, (time 1.4009366035461426)\n",
      "Training loss: 34.31187057495117 ¦ Validation -- loss: 0.6484580039978027, true accuracy: 0.5199999809265137, Bernoulli accuracy: 0.47999998927116394,  at epoch 55, (time 1.3839573860168457)\n",
      "Training loss: 33.36241912841797 ¦ Validation -- loss: 0.6380281448364258, true accuracy: 0.550000011920929, Bernoulli accuracy: 0.550000011920929,  at epoch 60, (time 1.382889986038208)\n",
      "Training loss: 31.21657943725586 ¦ Validation -- loss: 0.6483994126319885, true accuracy: 0.49000000953674316, Bernoulli accuracy: 0.49000000953674316,  at epoch 65, (time 1.3867738246917725)\n",
      "Training loss: 32.29867935180664 ¦ Validation -- loss: 0.6350096464157104, true accuracy: 0.46000000834465027, Bernoulli accuracy: 0.46000000834465027,  at epoch 70, (time 1.4046990871429443)\n",
      "Training loss: 33.571685791015625 ¦ Validation -- loss: 0.6248115301132202, true accuracy: 0.5199999809265137, Bernoulli accuracy: 0.47999998927116394,  at epoch 75, (time 1.413839340209961)\n",
      "Training loss: 29.245502471923828 ¦ Validation -- loss: 0.6264938712120056, true accuracy: 0.6100000143051147, Bernoulli accuracy: 0.6100000143051147,  at epoch 80, (time 1.378844976425171)\n",
      "Training loss: 30.246171951293945 ¦ Validation -- loss: 0.6282629370689392, true accuracy: 0.49000000953674316, Bernoulli accuracy: 0.49000000953674316,  at epoch 85, (time 1.3836884498596191)\n",
      "Training loss: 30.15308952331543 ¦ Validation -- loss: 0.615333080291748, true accuracy: 0.5099999904632568, Bernoulli accuracy: 0.5099999904632568,  at epoch 90, (time 1.3833849430084229)\n",
      "Training loss: 30.646224975585938 ¦ Validation -- loss: 0.5852013230323792, true accuracy: 0.5, Bernoulli accuracy: 0.5,  at epoch 95, (time 1.3909764289855957)\n",
      "Training loss: 29.947383880615234 ¦ Validation -- loss: 0.6085653305053711, true accuracy: 0.550000011920929, Bernoulli accuracy: 0.550000011920929,  at epoch 100, (time 1.410252332687378)\n"
     ]
    }
   ],
   "source": [
    "opt='readout'\n",
    "#results = '/mnt/d/ABG_Projects_Backup/ssn_modelling/ssn-simulator/results/testing/smallinit_poc_model.csv'\n",
    "new_ssn_layer_pars, new_readout_pars, val_loss_test, training_los_test= separate_train_SSN_vmap(ssn_layer_pars, readout_pars, ssn_pars, grid_pars, conn_pars, stimuli_pars, filter_pars,  conv_pars, loss_pars, epochs_to_save=epochs_to_save, opt=opt,  results_filename=None, ref_ori = 55, offset = 2, batch_size = 50, epochs = epochs, early_stop = 0.7, sig_noise=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2703dd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1234.7322\n",
      "-14276.857\n",
      "7187.562\n",
      "43323.58\n",
      "-12848.738\n",
      "-4661.069\n",
      "-1356.0579\n",
      "-251.61371\n",
      "17160.04\n",
      "-11464.824\n",
      "-3088.9595\n",
      "-1585.5419\n",
      "-1027.9004\n",
      "133.42638\n",
      "-31768.967\n",
      "838.277\n",
      "621.3622\n",
      "-2381.8818\n",
      "-456.7565\n",
      "-2329.2063\n",
      "-2246.3767\n",
      "-6866.311\n",
      "-3145.1875\n",
      "-7724.4683\n",
      "4266.7065\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(new_readout_pars['w_sig'])):\n",
    "    print((new_readout_pars['w_sig'][i] / readout_pars['w_sig'][i] - 1 )*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f058cbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training  -- loss: 0.71844083070755, true accuracy: 0.699999988079071, Bernoulli accuracy: 0.30000001192092896 \n",
      "Saving results to csv  /mnt/d/ABG_Projects_Backup/ssn_modelling/ssn-simulator/results/testing/sig_noise5_zeroinit_ssn.csv\n",
      "Training loss: 6.994512557983398 ¦ Validation -- loss: 0.7404026985168457, true accuracy: 0.5, Bernoulli accuracy: 0.5,  at epoch 1, (time 25.101052045822144)\n",
      "Training loss: 7.932202339172363 ¦ Validation -- loss: 0.7185806632041931, true accuracy: 0.5, Bernoulli accuracy: 0.5,  at epoch 5, (time 23.666568756103516)\n",
      "Training loss: 6.94173526763916 ¦ Validation -- loss: 0.7223901152610779, true accuracy: 0.4000000059604645, Bernoulli accuracy: 0.4000000059604645,  at epoch 10, (time 24.238095998764038)\n",
      "Training loss: 7.088151454925537 ¦ Validation -- loss: 0.7027559280395508, true accuracy: 0.5, Bernoulli accuracy: 0.5,  at epoch 15, (time 24.26532006263733)\n",
      "Training loss: 6.907599925994873 ¦ Validation -- loss: 0.689742922782898, true accuracy: 0.5, Bernoulli accuracy: 0.5,  at epoch 20, (time 23.41147518157959)\n",
      "Training loss: 6.7192277908325195 ¦ Validation -- loss: 0.67877197265625, true accuracy: 0.5, Bernoulli accuracy: 0.5,  at epoch 25, (time 23.54395604133606)\n",
      "Training loss: 6.929862022399902 ¦ Validation -- loss: 0.6487783789634705, true accuracy: 0.699999988079071, Bernoulli accuracy: 0.699999988079071,  at epoch 30, (time 23.31099557876587)\n",
      "Training loss: 6.671154975891113 ¦ Validation -- loss: 0.6786078810691833, true accuracy: 0.6000000238418579, Bernoulli accuracy: 0.4000000059604645,  at epoch 35, (time 23.47493076324463)\n",
      "Training loss: 6.597959041595459 ¦ Validation -- loss: 0.6488449573516846, true accuracy: 0.800000011920929, Bernoulli accuracy: 0.800000011920929,  at epoch 40, (time 23.766969680786133)\n",
      "Training loss: 6.795413017272949 ¦ Validation -- loss: 0.6706326603889465, true accuracy: 0.5, Bernoulli accuracy: 0.5,  at epoch 45, (time 23.475825548171997)\n",
      "Training loss: 6.734643459320068 ¦ Validation -- loss: 0.7143653631210327, true accuracy: 0.4000000059604645, Bernoulli accuracy: 0.4000000059604645,  at epoch 50, (time 23.35313105583191)\n",
      "Training loss: 6.777782917022705 ¦ Validation -- loss: 0.6591785550117493, true accuracy: 0.6000000238418579, Bernoulli accuracy: 0.6000000238418579,  at epoch 55, (time 23.217978715896606)\n",
      "Training loss: 6.686018466949463 ¦ Validation -- loss: 0.6514729261398315, true accuracy: 0.6000000238418579, Bernoulli accuracy: 0.6000000238418579,  at epoch 60, (time 22.99948501586914)\n",
      "Training loss: 7.165040969848633 ¦ Validation -- loss: 0.6774895191192627, true accuracy: 0.30000001192092896, Bernoulli accuracy: 0.30000001192092896,  at epoch 65, (time 23.457841396331787)\n",
      "Training loss: 6.487874984741211 ¦ Validation -- loss: 0.6725221276283264, true accuracy: 0.699999988079071, Bernoulli accuracy: 0.699999988079071,  at epoch 70, (time 22.414340496063232)\n",
      "Training loss: 6.7232513427734375 ¦ Validation -- loss: 0.673487663269043, true accuracy: 0.5, Bernoulli accuracy: 0.5,  at epoch 75, (time 24.029057264328003)\n",
      "Training loss: 6.767054080963135 ¦ Validation -- loss: 0.6696431040763855, true accuracy: 0.6000000238418579, Bernoulli accuracy: 0.6000000238418579,  at epoch 80, (time 23.167263507843018)\n",
      "Training loss: 6.709648609161377 ¦ Validation -- loss: 0.6728178262710571, true accuracy: 0.5, Bernoulli accuracy: 0.5,  at epoch 85, (time 23.72114133834839)\n",
      "Training loss: 7.081052780151367 ¦ Validation -- loss: 0.6791931986808777, true accuracy: 0.4000000059604645, Bernoulli accuracy: 0.4000000059604645,  at epoch 90, (time 23.119332551956177)\n",
      "Training loss: 6.731490135192871 ¦ Validation -- loss: 0.6723611354827881, true accuracy: 0.699999988079071, Bernoulli accuracy: 0.30000001192092896,  at epoch 95, (time 23.56767964363098)\n",
      "Training loss: 6.731225967407227 ¦ Validation -- loss: 0.711137592792511, true accuracy: 0.30000001192092896, Bernoulli accuracy: 0.30000001192092896,  at epoch 100, (time 23.36614227294922)\n",
      "Training loss: 6.783841133117676 ¦ Validation -- loss: 0.7023738026618958, true accuracy: 0.30000001192092896, Bernoulli accuracy: 0.30000001192092896,  at epoch 105, (time 24.035626649856567)\n",
      "Training loss: 6.935623645782471 ¦ Validation -- loss: 0.6767956614494324, true accuracy: 0.5, Bernoulli accuracy: 0.5,  at epoch 110, (time 24.29943537712097)\n",
      "Training loss: 6.754578113555908 ¦ Validation -- loss: 0.6454222798347473, true accuracy: 0.699999988079071, Bernoulli accuracy: 0.699999988079071,  at epoch 115, (time 24.38737726211548)\n",
      "Training loss: 6.759406566619873 ¦ Validation -- loss: 0.6837865114212036, true accuracy: 0.4000000059604645, Bernoulli accuracy: 0.4000000059604645,  at epoch 120, (time 25.13841676712036)\n",
      "Training loss: 6.722266674041748 ¦ Validation -- loss: 0.686466634273529, true accuracy: 0.4000000059604645, Bernoulli accuracy: 0.4000000059604645,  at epoch 125, (time 24.495831966400146)\n",
      "Training loss: 6.711245536804199 ¦ Validation -- loss: 0.6775663495063782, true accuracy: 0.5, Bernoulli accuracy: 0.5,  at epoch 130, (time 23.589862823486328)\n",
      "Training loss: 6.430568695068359 ¦ Validation -- loss: 0.6463126540184021, true accuracy: 0.699999988079071, Bernoulli accuracy: 0.699999988079071,  at epoch 135, (time 24.29737114906311)\n",
      "Training loss: 6.672375679016113 ¦ Validation -- loss: 0.6959921717643738, true accuracy: 0.4000000059604645, Bernoulli accuracy: 0.4000000059604645,  at epoch 140, (time 24.293400049209595)\n",
      "Training loss: 6.635139465332031 ¦ Validation -- loss: 0.6646555066108704, true accuracy: 0.699999988079071, Bernoulli accuracy: 0.699999988079071,  at epoch 145, (time 23.263131380081177)\n",
      "Training loss: 6.793154716491699 ¦ Validation -- loss: 0.678180992603302, true accuracy: 0.30000001192092896, Bernoulli accuracy: 0.699999988079071,  at epoch 150, (time 23.47025990486145)\n",
      "Training loss: 6.745127201080322 ¦ Validation -- loss: 0.6617639660835266, true accuracy: 0.6000000238418579, Bernoulli accuracy: 0.6000000238418579,  at epoch 155, (time 23.139740228652954)\n",
      "Training loss: 6.7045793533325195 ¦ Validation -- loss: 0.6614378690719604, true accuracy: 0.6000000238418579, Bernoulli accuracy: 0.6000000238418579,  at epoch 160, (time 23.553971767425537)\n",
      "Training loss: 6.99350118637085 ¦ Validation -- loss: 0.6812028288841248, true accuracy: 0.4000000059604645, Bernoulli accuracy: 0.4000000059604645,  at epoch 165, (time 23.494144678115845)\n",
      "Training loss: 6.754175186157227 ¦ Validation -- loss: 0.6827906966209412, true accuracy: 0.5, Bernoulli accuracy: 0.5,  at epoch 170, (time 23.291645765304565)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_ssn = '/mnt/d/ABG_Projects_Backup/ssn_modelling/ssn-simulator/results/testing/sig_noise5_zeroinit_ssn.csv'\n",
    "opt='ssn'\n",
    "final_ssn_layer_pars, final_readout_pars, val_loss_test, training_los_test= separate_train_SSN_vmap(ssn_layer_pars, new_readout_pars, ssn_pars, grid_pars, conn_pars, stimuli_pars, filter_pars,  conv_pars, loss_pars, epochs_to_save=epochs_to_save, opt=opt,  results_filename=results_ssn, ref_ori = 55, offset = 10, batch_size = 10, epochs = epochs, sig_noise = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b7ff3d",
   "metadata": {},
   "source": [
    "## Proof of concept model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a3dec2",
   "metadata": {},
   "source": [
    "Using trained readout layer -> quantify effects of changing different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c97d1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = create_data(stimuli_pars, number = 500, offset = 10, ref_ori = 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a014005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(1.5467694, dtype=float32),\n",
       " DeviceArray(0.246, dtype=float32),\n",
       " DeviceArray(0.434, dtype=float32))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Js0 = [1.82650658, 0.681944750, 2.06815311, 0.5106321]\n",
    "J_2x2 = make_J2x2(*Js0)\n",
    "ssn_layer_pars['logJ_2x2'] = np.log(J_2x2*signs)\n",
    "\n",
    "val_loss, true_acc, ber_acc= vmap_eval(ssn_layer_pars, new_readout_pars, ssn_pars, grid_pars, conn_pars, test_data, filter_pars,  conv_pars, loss_pars, bernoulli=True, sig_noise=2)\n",
    "val_loss, true_acc, ber_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "967c8eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([ 0.00253614,  0.00701615,  0.0044661 , -0.00820982,\n",
       "             -0.00665725, -0.00724147, -0.00163709,  0.00058696,\n",
       "              0.00932288,  0.00519365, -0.00377257, -0.00610754,\n",
       "             -0.003521  , -0.00089833,  0.00654265, -0.00065734,\n",
       "              0.00225603, -0.00777152,  0.00156693, -0.00620367,\n",
       "              0.00239886, -0.00755739,  0.00424556, -0.00892564,\n",
       "             -0.0015102 ], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_readout_pars['w_sig']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc84b458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([ 0.09492798, -0.0765982 ,  0.17109391,  0.52019674,\n",
       "             -0.15374203, -0.37655032, -0.17721649,  0.09932156,\n",
       "              0.3284847 ,  0.03279679,  0.10647322, -0.10445496,\n",
       "              0.44695792, -0.03648688,  0.37867212, -0.15572803,\n",
       "             -0.17950897, -0.29801497,  0.25172082,  0.20971169,\n",
       "              0.05513088,  0.23197745, -0.12729205,  0.25461578,\n",
       "              0.0877445 ], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_readout_pars['w_sig'] # weights 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
