{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f6c9db",
   "metadata": {},
   "source": [
    "## TRAINING IMPLEMENTATION "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0482e672",
   "metadata": {},
   "source": [
    "In this notebook: \n",
    "- training script to be run on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b778e5e",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a96f7e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time, os, json\n",
    "import pandas as pd\n",
    "from scipy import stats \n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import jax\n",
    "from jax import random\n",
    "from jax.config import config \n",
    "import jax.numpy as np\n",
    "from jax import vmap\n",
    "import pdb\n",
    "import optax\n",
    "import time\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy\n",
    "import torch\n",
    "\n",
    "#config.update('jax_debug_nans', True)\n",
    "from SSN_classes_jax import SSN2DTopoV1_AMPAGABA_ONOFF\n",
    "from util import GaborFilter, BW_Grating, find_A, create_gabor_filters, create_gratings\n",
    "\n",
    "#initialize key\n",
    "key = random.PRNGKey(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735f18bf",
   "metadata": {},
   "source": [
    "->Check GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "769bbfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "[CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71f005",
   "metadata": {},
   "source": [
    "## 2. PARAMETERS TO DEFINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246fdcd9",
   "metadata": {},
   "source": [
    "### 2.1 Stimuli parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c78a1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gabor parameters \n",
    "sigma_g= 0.5\n",
    "k = np.pi/(6*sigma_g)\n",
    "\n",
    "#Stimuli parameters\n",
    "ref_ori = 55\n",
    "offset = 5\n",
    "\n",
    "#Assemble parameters in dictionary\n",
    "general_pars = dict(k=k , edge_deg=3.2,  degree_per_pixel=0.05)\n",
    "stimuli_pars = dict(outer_radius=3, inner_radius=2.5, grating_contrast=0.8, std = 15, jitter_val = 5, snr = 1)\n",
    "stimuli_pars.update(general_pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def46ec0",
   "metadata": {},
   "source": [
    "### 2.2. Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2864d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network parameters\n",
    "class ssn_pars():\n",
    "    n = 2\n",
    "    k = 0.04\n",
    "    tauE = 30 # in ms\n",
    "    tauI = 10 # in ms~\n",
    "    psi = 0.774\n",
    "    tau_s = np.array([5, 7, 100]) #in ms, AMPA, GABA, NMDA current decay time constants\n",
    "    \n",
    "\n",
    "#Grid parameters\n",
    "class grid_pars():\n",
    "    gridsize_Nx = 9 # grid-points across each edge # gives rise to dx = 0.8 mm\n",
    "    gridsize_deg = 2 * 1.6 # edge length in degrees\n",
    "    magnif_factor = 2  # mm/deg\n",
    "    hyper_col = 0.8 # mm   \n",
    "    sigma_RF = 0.4 # deg (visual angle)\n",
    "\n",
    "# Caleb's params for the full (with local) model:\n",
    "Js0 = [1.82650658, 0.68194475, 2.06815311, 0.5106321]\n",
    "gE, gI = 0.57328625, 0.26144141\n",
    "\n",
    "sigEE, sigIE = 0.2, 0.40\n",
    "sigEI, sigII = .09, .09\n",
    "conn_pars = dict(\n",
    "    PERIODIC = False,\n",
    "    p_local = [.4, 0.7], # [p_local_EE, p_local_IE],\n",
    "    sigma_oris = 1000) # sigma_oris\n",
    "\n",
    "\n",
    "make_J2x2 = lambda Jee, Jei, Jie, Jii: np.array([[Jee, -Jei], [Jie,  -Jii]]) * np.pi * ssn_pars.psi\n",
    "J_2x2 = make_J2x2(*Js0)\n",
    "s_2x2 = np.array([[sigEE, sigEI],[sigIE, sigII]])\n",
    "\n",
    "#Positive reparameterization\n",
    "signs=np.array([[1, -1], [1, -1]])\n",
    "logJ_2x2 =np.log(J_2x2*signs)\n",
    "logs_2x2 = np.log(s_2x2)\n",
    "\n",
    "\n",
    "#Sigmoid parameters~\n",
    "N_neurons = 25\n",
    "\n",
    "key, _ = random.split(key)\n",
    "w_sig = random.normal(key, shape = (N_neurons,)) / np.sqrt(N_neurons)\n",
    "b_sig = 0.0\n",
    "\n",
    "#Excitatory and inhibitory constants for extra synaptic GABA\n",
    "c_E = 0.0008299054* 5 /100# 1.0\n",
    "c_I = 0.0008299054* 5 /100\n",
    "\n",
    "\n",
    "#Optimization pars\n",
    "opt_pars = dict(logJ_2x2 = logJ_2x2, logs_2x2 = logs_2x2, w_sig = w_sig, b_sig=b_sig)\n",
    "\n",
    "\n",
    "#Parameters exclusive to Gabor filters\n",
    "filter_pars = dict(sigma_g = sigma_g, conv_factor = grid_pars.magnif_factor)\n",
    "filter_pars.update(general_pars) \n",
    "\n",
    "#Convergence parameters\n",
    "conv_pars=dict(dt = 1, xtol = 1e-5, Tmax = 200, verbose=False, silent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf068b",
   "metadata": {},
   "source": [
    "### 2.3 Training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4693394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/temp/ssn_modelling/ssn-simulator/results/Tmax400.csv\n"
     ]
    }
   ],
   "source": [
    "#Name of results csv\n",
    "home_dir = os.getcwd()\n",
    "#Create directory for results\n",
    "results_dir = os.path.join(home_dir, 'results')\n",
    "if os.path.exists(results_dir) == False:\n",
    "        os.makedirs(results_dir)\n",
    "        \n",
    "        \n",
    "results_name = 'Tmax400.csv' #SPECIFY NAME OF RESULTS FILE\n",
    "if results_name == None:\n",
    "    results_name = 'results.csv'\n",
    "\n",
    "results_filename = os.path.join(results_dir, results_name)\n",
    "print(results_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2f726a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of epochs\n",
    "epochs = 200\n",
    "num_epochs_to_save = 21\n",
    "epochs_to_save = np.linspace(1 ,epochs, num_epochs_to_save).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c92551",
   "metadata": {},
   "source": [
    "## 3. TRAINING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffeeb43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(stimuli_pars, number=100, offset = 5, ref_ori=55):\n",
    "    \n",
    "    '''\n",
    "    Create data for given jitter and snr value for testing (not dataloader)\n",
    "    '''\n",
    "    data = create_gratings(ref_ori=ref_ori, number=number, offset=offset, **stimuli_pars)\n",
    "    train_data = next(iter(DataLoader(data, batch_size=len(data), shuffle=False)))\n",
    "    train_data['ref'] = train_data['ref'].numpy()\n",
    "    train_data['target'] = train_data['target'].numpy()\n",
    "    train_data['label'] = train_data['label'].numpy()\n",
    "    \n",
    "    return train_data\n",
    "\n",
    "\n",
    "def constant_to_vec(c_E, c_I):\n",
    "    \n",
    "    matrix_E = np.zeros((9,9))\n",
    "    matrix_E = matrix_E.at[2:7, 2:7].set(c_E)\n",
    "    vec_E = np.ravel(matrix_E)\n",
    "    \n",
    "    matrix_I = np.zeros((9,9))\n",
    "    matrix_I = matrix_I.at[2:7, 2:7].set(c_I)\n",
    "    vec_I = np.ravel(matrix_I)\n",
    "    \n",
    "    constant_vec = np.hstack((vec_E, vec_E, vec_I, vec_I))\n",
    "    return constant_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b12438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(x))\n",
    "\n",
    "def binary_loss(n, x):\n",
    "    return - (n*np.log(x) + (1-n)*np.log(1-x))\n",
    "\n",
    "def model(opt_pars, ssn_pars, grid_pars, conn_pars, train_data, filter_pars,  conv_pars):\n",
    "    \n",
    "    signs=np.array([[1, -1], [1, -1]])\n",
    "    \n",
    "    J_2x2 =np.exp(opt_pars['logJ_2x2'])*signs\n",
    "    s_2x2 = np.exp(opt_pars['logs_2x2'])\n",
    "    \n",
    "    #Initialise network\n",
    "    ssn=SSN2DTopoV1_AMPAGABA_ONOFF(ssn_pars=ssn_pars, grid_pars=grid_pars, conn_pars=conn_pars, filter_pars=filter_pars, J_2x2=J_2x2, s_2x2=s_2x2)\n",
    "    \n",
    "    #Create vector using extrasynaptic constants\n",
    "    #constant_vector = constant_to_vec(opt_pars['c_E'], opt_pars['c_I'])\n",
    "    \n",
    "    #Apply Gabor filters to stimuli\n",
    "    output_ref=np.matmul(ssn.gabor_filters, train_data['ref'])*ssn.A #+ constant_vector\n",
    "    output_target=np.matmul(ssn.gabor_filters, train_data['target'])*ssn.A #+ constant_vector\n",
    "    \n",
    "    print('output_ref mean', str(output_ref.mean()))\n",
    "    #Rectify output\n",
    "    SSN_input_ref=np.maximum(0, output_ref)\n",
    "    SSN_input_target=np.maximum(0, output_target)\n",
    "\n",
    "    #Input to SSN\n",
    "    r_init = np.zeros(SSN_input_ref.shape[0])\n",
    "\n",
    "    fp_ref, _, _ = ssn.fixed_point_r(SSN_input_ref, r_init=r_init, **conv_pars)\n",
    "    print('max rate ', np.array(fp_ref.max()))\n",
    "    x_ref = ssn.apply_bounding_box(fp_ref, size=3.2)\n",
    "\n",
    "    fp_target, _, _ = ssn.fixed_point_r(SSN_input_target, r_init=r_init, **conv_pars)\n",
    "    x_target = ssn.apply_bounding_box(fp_target, size=3.2)\n",
    "\n",
    "    #Apply sigmoid function - combine ref and target\n",
    "    x = sigmoid( np.dot(opt_pars['w_sig'], (x_ref.ravel() - x_target.ravel())) + opt_pars['b_sig'])\n",
    "\n",
    "    #Calculate binary cross entropy loss\n",
    "    loss=binary_loss(train_data['label'], x)\n",
    "   \n",
    "    return loss\n",
    "\n",
    "\n",
    "def loss(opt_pars, ssn_pars, grid_pars, conn_pars, train_data, filter_pars,  conv_pars):\n",
    "    '''\n",
    "    Calculate parallelized loss for batch of data through vmap.\n",
    "    Output:\n",
    "        mean loss of all the input images\n",
    "    '''\n",
    "    \n",
    "    vmap_model = vmap(model, in_axes = ({'b_sig': None, 'logJ_2x2': None, 'logs_2x2': None, 'w_sig': None}, None, None, {'PERIODIC': None, 'p_local': [None, None], 'sigma_oris': None},  {'ref':0, 'target':0, 'label':0}, {'conv_factor': None, 'degree_per_pixel': None, 'edge_deg': None, 'k': None, 'sigma_g': None}, {'Tmax': None, 'dt': None, 'silent': None, 'verbose': None, 'xtol': None}) )                   \n",
    "    loss = np.sum(vmap_model(opt_pars, ssn_pars, grid_pars, conn_pars, train_data, filter_pars,  conv_pars))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def eval_model(opt_pars, ssn_pars, grid_pars, conn_pars, test_data, filter_pars,  conv_pars):\n",
    "    signs=np.array([[1, -1], [1, -1]])\n",
    "    \n",
    "    J_2x2 =np.exp(opt_pars['logJ_2x2'])*signs\n",
    "    s_2x2 = np.exp(opt_pars['logs_2x2'])\n",
    "    \n",
    "    #Initialise network\n",
    "    ssn=SSN2DTopoV1_AMPAGABA_ONOFF(ssn_pars=ssn_pars, grid_pars=grid_pars, conn_pars=conn_pars, filter_pars=filter_pars, J_2x2=J_2x2, s_2x2=s_2x2)\n",
    "\n",
    "    #Apply Gabor filters to stimuli\n",
    "    output_ref=np.matmul(ssn.gabor_filters, test_data['ref'])*ssn.A\n",
    "    output_target=np.matmul(ssn.gabor_filters, test_data['target'])*ssn.A\n",
    "\n",
    "    #Rectify output\n",
    "    SSN_input_ref=np.maximum(0, output_ref)\n",
    "    SSN_input_target=np.maximum(0, output_target)\n",
    "\n",
    "    #Input to SSN\n",
    "    r_init = np.zeros(SSN_input_ref.shape[0])\n",
    "\n",
    "    fp_ref, _, _= ssn.fixed_point_r(SSN_input_ref, r_init=r_init, **conv_pars)\n",
    "    x_ref = ssn.apply_bounding_box(fp_ref, size=3.2)\n",
    "\n",
    "    fp_target, _, _ = ssn.fixed_point_r(SSN_input_target, r_init=r_init, **conv_pars)\n",
    "    x_target = ssn.apply_bounding_box(fp_target, size=3.2)\n",
    "    \n",
    "    dot= np.dot(opt_pars['w_sig'], (x_ref.ravel() - x_target.ravel()))\n",
    "\n",
    "    #Apply sigmoid function - combine ref and target\n",
    "    x = sigmoid( np.dot(opt_pars['w_sig'], (x_ref.ravel() - x_target.ravel())) + opt_pars['b_sig'])\n",
    "    \n",
    "    #compare prediction to label\n",
    "    pred_label = np.round(x)\n",
    "\n",
    "    #Calculate binary cross entropy loss\n",
    "    loss=binary_loss(test_data['label'], x)\n",
    "    \n",
    "    return loss, pred_label, x\n",
    "\n",
    "\n",
    "def vmap_eval(opt_pars, ssn_pars, grid_pars, conn_pars, test_data, filter_pars,  conv_pars):\n",
    "    \n",
    "    eval_vmap = vmap(eval_model, in_axes = ({'b_sig': None,  'logJ_2x2': None, 'logs_2x2': None, 'w_sig': None}, None, None, {'PERIODIC': None, 'p_local': [None, None], 'sigma_oris': None},  {'ref':0, 'target':0, 'label':0}, {'conv_factor': None, 'degree_per_pixel': None, 'edge_deg': None, 'k': None, 'sigma_g': None}, {'Tmax': None, 'dt': None, 'silent': None, 'verbose': None, 'xtol': None}) )\n",
    "    losses, pred_labels, dots = eval_vmap(opt_pars, ssn_pars, grid_pars, conn_pars, test_data, filter_pars,  conv_pars)\n",
    "    \n",
    "    accuracy = np.sum(test_data['label'] == pred_labels)/len(test_data['label']) \n",
    "    \n",
    "    vmap_loss= np.mean(losses)\n",
    "    \n",
    "    return vmap_loss, accuracy, dots\n",
    "\n",
    "\n",
    "def train_SSN_vmap(opt_pars, ssn_pars, grid_pars, conn_pars, stimuli_pars, filter_pars, conv_pars, results_filename, epochs_to_save, batch_size=20, ref_ori = 55, offset = 5, epochs=1, eta=10e-4):\n",
    "    \n",
    "    #Initialize loss\n",
    "    val_loss_per_epoch = []\n",
    "    training_losses=[]\n",
    "    accuracies=[]\n",
    "        \n",
    "    #Initialise optimizer\n",
    "    optimizer = optax.adam(eta)\n",
    "    opt_state = optimizer.init(opt_pars)\n",
    "    print('Training model for {} epochs at reference orientation {} and offset {} (batch size = {} )'.format(epochs, ref_ori, offset, batch_size))\n",
    "    \n",
    "    #Define test data - no need to iterate\n",
    "    test_data = create_data(stimuli_pars, number = batch_size, offset = offset, ref_ori = ref_ori)\n",
    "    val_loss, accuracy, _= vmap_eval(opt_pars, ssn_pars, grid_pars, conn_pars, test_data, filter_pars,  conv_pars)\n",
    "    print('Before training  -- loss: {}, accuracy: {} '.format(val_loss, accuracy))\n",
    "    val_loss_per_epoch.append(val_loss)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    results_handle = open(results_filename, 'w')\n",
    "    results_writer=None\n",
    "    \n",
    "\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0 \n",
    "           \n",
    "        #load next batch of data and convert\n",
    "        train_data = create_data(stimuli_pars, number = batch_size, offset = offset, ref_ori = ref_ori)\n",
    "\n",
    "        #Compute loss and gradient\n",
    "        epoch_loss, grad =jax.value_and_grad(loss)(opt_pars, ssn_pars, grid_pars, conn_pars, train_data, filter_pars,  conv_pars)\n",
    "\n",
    "        #Apply SGD through Adam optimizer per batch\n",
    "        updates, opt_state = optimizer.update(grad, opt_state)\n",
    "        opt_pars = optax.apply_updates(opt_pars, updates)\n",
    "        training_losses.append(epoch_loss)\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        #Save the parameters given a number of epochs\n",
    "        if epoch in epochs_to_save:\n",
    "            \n",
    "            #Evaluate model \n",
    "            test_data = create_data(stimuli_pars, number = batch_size, offset = offset, ref_ori = ref_ori)\n",
    "            val_loss, accuracy, _= vmap_eval(opt_pars, ssn_pars, grid_pars, conn_pars, test_data, filter_pars,  conv_pars)\n",
    "            print('Training loss: {} ¦ Validation -- loss: {}, accuracy: {} at epoch {}, (time {})'.format(epoch_loss, val_loss, accuracy, epoch, epoch_time))\n",
    "            val_loss_per_epoch.append(val_loss)\n",
    "            accuracies.append(accuracy)\n",
    "            \n",
    "            #Create dictionary to save desired params\n",
    "            save_params= dict(val_accuracy= accuracy, \n",
    "                J_EE= opt_pars['logJ_2x2'][0,0], J_EI = opt_pars['logJ_2x2'][0,1], \n",
    "                              J_IE = opt_pars['logJ_2x2'][1,0], J_II = opt_pars['logJ_2x2'][1,1], \n",
    "                s_EE= opt_pars['logs_2x2'][0,0], s_EI = opt_pars['logs_2x2'][0,1], \n",
    "                              s_IE = opt_pars['logs_2x2'][1,0], s_II = opt_pars['logs_2x2'][1,1],\n",
    "                #c_E = opt_pars['c_E'], c_I = opt_pars['c_I'], \n",
    "                 epoch = epoch)\n",
    "            \n",
    "            #Only write header once\n",
    "            #if results_writer is None:\n",
    "                #results_writer = csv.DictWriter(results_handle, fieldnames=save_params.keys())\n",
    "               # results_writer.writeheader()\n",
    "            \n",
    "            #Write results in csv file\n",
    "            #results_writer.writerow(save_params)\n",
    "         \n",
    "    \n",
    "    #Reparametize parameters\n",
    "    signs=np.array([[1, -1], [1, -1]])    \n",
    "    opt_pars['logJ_2x2'] = np.exp(opt_pars['logJ_2x2'])*signs\n",
    "    opt_pars['logs_2x2'] = np.exp(opt_pars['logs_2x2'])\n",
    "    \n",
    "   \n",
    "    return opt_pars, val_loss_per_epoch, training_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9826b188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for 200 epochs at reference orientation 55 and offset 2 (batch size = 50 )\n",
      "Before training  -- loss: 0.6931539177894592, accuracy: 0.0 \n",
      "output_ref mean Traced<ShapedArray(float32[])>with<BatchTrace(level=3/0)> with\n",
      "  val = DeviceArray([-9.19824772e-11, -4.59912386e-11, -9.19824772e-11,\n",
      "              1.83964954e-10,  6.89868579e-11,  6.89868579e-11,\n",
      "             -4.59912386e-11, -4.59912386e-11,  1.83964954e-10,\n",
      "              2.29956193e-11,  1.83964954e-10,  2.75947432e-10,\n",
      "             -1.14978096e-10,  9.19824772e-11, -4.59912386e-11,\n",
      "             -9.19824772e-11,  1.37973716e-10,  1.14978096e-10,\n",
      "              4.59912386e-11,  0.00000000e+00,  1.83964954e-10,\n",
      "              0.00000000e+00, -2.29956193e-11, -1.14978096e-10,\n",
      "              0.00000000e+00,  4.59912386e-11, -2.29956193e-11,\n",
      "             -2.29956193e-11,  2.29956193e-11,  1.37973716e-10,\n",
      "              9.19824772e-11, -1.83964954e-10, -1.83964954e-10,\n",
      "              4.59912386e-11,  2.29956193e-11,  2.29956193e-10,\n",
      "              0.00000000e+00,  1.83964954e-10, -1.37973716e-10,\n",
      "              9.19824772e-11, -1.37973716e-10,  2.29956193e-11,\n",
      "              2.29956193e-10,  0.00000000e+00, -9.19824772e-11,\n",
      "             -4.59912386e-11, -2.29956193e-11,  0.00000000e+00,\n",
      "              4.59912386e-11,  2.29956193e-11], dtype=float32)\n",
      "  batch_dim = 0\n",
      "max rate  Traced<ShapedArray(float32[])>with<BatchTrace(level=3/0)> with\n",
      "  val = Traced<ConcreteArray([0.00012642 0.00015979 0.00013843 0.00014947 0.00013251 0.00014364\n",
      " 0.00013061 0.0001637  0.00015792 0.00012954 0.00015987 0.00016017\n",
      " 0.00012708 0.00015602 0.00014388 0.00015966 0.00015474 0.00015319\n",
      " 0.00013437 0.000158   0.00015504 0.00016171 0.00012633 0.00012925\n",
      " 0.00015912 0.00012592 0.00013006 0.00012731 0.00012976 0.00016124\n",
      " 0.00016647 0.00016068 0.00015961 0.00016451 0.00013126 0.00015361\n",
      " 0.00013116 0.00016332 0.00016384 0.00016261 0.00014716 0.00012305\n",
      " 0.00015665 0.00015417 0.00014591 0.0001657  0.00012915 0.00014134\n",
      " 0.00016069 0.00012375], dtype=float32)>with<JVPTrace(level=2/0)> with\n",
      "    primal = DeviceArray([0.00012642, 0.00015979, 0.00013843, 0.00014947, 0.00013251,\n",
      "             0.00014364, 0.00013061, 0.0001637 , 0.00015792, 0.00012954,\n",
      "             0.00015987, 0.00016017, 0.00012708, 0.00015602, 0.00014388,\n",
      "             0.00015966, 0.00015474, 0.00015319, 0.00013437, 0.000158  ,\n",
      "             0.00015504, 0.00016171, 0.00012633, 0.00012925, 0.00015912,\n",
      "             0.00012592, 0.00013006, 0.00012731, 0.00012976, 0.00016124,\n",
      "             0.00016647, 0.00016068, 0.00015961, 0.00016451, 0.00013126,\n",
      "             0.00015361, 0.00013116, 0.00016332, 0.00016384, 0.00016261,\n",
      "             0.00014716, 0.00012305, 0.00015665, 0.00015417, 0.00014591,\n",
      "             0.0001657 , 0.00012915, 0.00014134, 0.00016069, 0.00012375],            dtype=float32)\n",
      "    tangent = Traced<ShapedArray(float32[50])>with<JaxprTrace(level=1/0)> with\n",
      "      pval = (ShapedArray(float32[50]), None)\n",
      "      recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7fa48869cfe0>, in_tracers=(Traced<ShapedArray(float32[50]):JaxprTrace(level=1/0)>,), out_tracer_refs=[<weakref at 0x7fa4a85cbd60; to 'JaxprTracer' at 0x7fa4a85fe900>], out_avals=[ShapedArray(float32[50])], primitive=copy, params={}, effects=set(), source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7fa4bc092170>, name_stack=NameStack(stack=(Transform(name='jvp'), Transform(name='vmap')))))\n",
      "  batch_dim = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train model\n",
    "final_pars, val_loss, training_loss = train_SSN_vmap(opt_pars, ssn_pars, grid_pars, conn_pars, stimuli_pars, filter_pars,  conv_pars, results_filename, epochs_to_save = epochs_to_save, ref_ori = 55, offset = 2, batch_size = 50, epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d760fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03226df5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
