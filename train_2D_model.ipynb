{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f6c9db",
   "metadata": {},
   "source": [
    "## TRAINING IMPLEMENTATION "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0482e672",
   "metadata": {},
   "source": [
    "In this notebook: \n",
    "- training script to be run on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b778e5e",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96f7e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time, os, json\n",
    "import pandas as pd\n",
    "from scipy import stats \n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import jax\n",
    "from jax import random\n",
    "from jax.config import config \n",
    "import jax.numpy as np\n",
    "from jax import vmap\n",
    "import pdb\n",
    "import optax\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy\n",
    "import torch\n",
    "\n",
    "#config.update('jax_debug_nans', True)\n",
    "from SSN_classes_jax import SSN2DTopoV1_AMPAGABA_ONOFF\n",
    "from util import GaborFilter, BW_Grating, find_A, create_gabor_filters, create_gratings\n",
    "\n",
    "#initialize key\n",
    "key = random.PRNGKey(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735f18bf",
   "metadata": {},
   "source": [
    "->Check GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769bbfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71f005",
   "metadata": {},
   "source": [
    "## 2. Stimuli parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c78a1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gabor parameters \n",
    "sigma_g= 0.5\n",
    "k = np.pi/(6*sigma_g)\n",
    "\n",
    "#Stimuli parameters\n",
    "ref_ori = 55\n",
    "offset = 5\n",
    "\n",
    "#Assemble parameters in dictionary\n",
    "general_pars = dict(k=k , edge_deg=3.2,  degree_per_pixel=0.05)\n",
    "stimuli_pars = dict(outer_radius=3, inner_radius=2.5, grating_contrast=0.8, std = 15, jitter_val = 5, snr = 1)\n",
    "stimuli_pars.update(general_pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def46ec0",
   "metadata": {},
   "source": [
    "## 3. Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2864d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network parameters\n",
    "class ssn_pars():\n",
    "    n = 2\n",
    "    k = 0.04\n",
    "    tauE = 30 # in ms\n",
    "    tauI = 10 # in ms~\n",
    "    psi = 0.774\n",
    "    tau_s = np.array([5, 7, 100]) #in ms, AMPA, GABA, NMDA current decay time constants\n",
    "    \n",
    "\n",
    "#Grid parameters\n",
    "class grid_pars():\n",
    "    gridsize_Nx = 9 # grid-points across each edge # gives rise to dx = 0.8 mm\n",
    "    gridsize_deg = 2 * 1.6 # edge length in degrees\n",
    "    magnif_factor = 2  # mm/deg\n",
    "    hyper_col = 0.8 # mm   \n",
    "    sigma_RF = 0.4 # deg (visual angle)\n",
    "\n",
    "# Caleb's params for the full (with local) model:\n",
    "Js0 = [1.82650658, 0.68194475, 2.06815311, 0.5106321]\n",
    "gE, gI = 0.57328625, 0.26144141\n",
    "\n",
    "sigEE, sigIE = 0.2, 0.40\n",
    "sigEI, sigII = .09, .09\n",
    "conn_pars = dict(\n",
    "    PERIODIC = False,\n",
    "    p_local = [.4, 0.7], # [p_local_EE, p_local_IE],\n",
    "    sigma_oris = 1000) # sigma_oris\n",
    "\n",
    "\n",
    "make_J2x2 = lambda Jee, Jei, Jie, Jii: np.array([[Jee, -Jei], [Jie,  -Jii]]) * np.pi * ssn_pars.psi\n",
    "J_2x2 = make_J2x2(*Js0)\n",
    "s_2x2 = np.array([[sigEE, sigEI],[sigIE, sigII]])\n",
    "\n",
    "#Positive reparameterization\n",
    "signs=np.array([[1, -1], [1, -1]])\n",
    "logJ_2x2 =np.log(J_2x2*signs)\n",
    "logs_2x2 = np.log(s_2x2)\n",
    "\n",
    "\n",
    "#Sigmoid parameters~\n",
    "N_neurons = 25\n",
    "\n",
    "key, _ = random.split(key)\n",
    "w_sig = random.normal(key, shape = (N_neurons,)) / np.sqrt(N_neurons)\n",
    "b_sig = 0.0\n",
    "\n",
    "#Excitatory and inhibitory constants for extra synaptic GABA\n",
    "c_E = 1.0\n",
    "c_I = 1.0\n",
    "\n",
    "#Optimization pars\n",
    "opt_pars = dict(logJ_2x2 = logJ_2x2, logs_2x2 = logs_2x2, w_sig = w_sig, b_sig=b_sig, c_E = c_E, c_I = c_I)\n",
    "\n",
    "\n",
    "#Parameters exclusive to Gabor filters\n",
    "filter_pars = dict(sigma_g = sigma_g, conv_factor = grid_pars.magnif_factor)\n",
    "filter_pars.update(general_pars) \n",
    "\n",
    "#Convergence parameters\n",
    "conv_pars=dict(dt = 1, xtol = 1e-5, Tmax = 200, verbose=False, silent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c92551",
   "metadata": {},
   "source": [
    "## 3. TRAINING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeeb43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(stimuli_pars, number=100, offset = 5, ref_ori=55):\n",
    "    \n",
    "    '''\n",
    "    Create data for given jitter and snr value for testing (not dataloader)\n",
    "    '''\n",
    "    data = create_gratings(ref_ori=ref_ori, number=number, offset=offset, **stimuli_pars)\n",
    "    train_data = next(iter(DataLoader(data, batch_size=len(data), shuffle=False)))\n",
    "    train_data['ref'] = train_data['ref'].numpy()\n",
    "    train_data['target'] = train_data['target'].numpy()\n",
    "    train_data['label'] = train_data['label'].numpy()\n",
    "    \n",
    "    return train_data\n",
    "\n",
    "def save_params(all_J, all_s, all_c, params, count):\n",
    "\n",
    "    all_J[0, count] = np.exp(params['logJ_2x2'][0,0]) #J_EE\n",
    "    all_J[1, count] = np.exp(params['logJ_2x2'][0,1])*-1 #J_EI\n",
    "    all_J[2, count] = np.exp(params['logJ_2x2'][1,0]) #J_IE\n",
    "    all_J[3, count] = np.exp(params['logJ_2x2'][1,1])*-1 #J_II\n",
    "    \n",
    "    all_s[0, count] = params['logs_2x2'][0,0] #s_EE\n",
    "    all_s[1, count] = params['logs_2x2'][0,1] #s_EI\n",
    "    all_s[2, count] = params['logs_2x2'][1,0] #s_IE\n",
    "    all_s[3, count] = params['logs_2x2'][1,1] #s_II\n",
    "    \n",
    "    all_c[0, count] = params['c_E']\n",
    "    all_c[1, count] = params ['c_I']\n",
    "\n",
    "    count +=1\n",
    "    \n",
    "    return all_J,all_s, all_c, count\n",
    "\n",
    "def constant_to_vec(c_E, c_I):\n",
    "    \n",
    "    matrix_E = np.zeros((9,9))\n",
    "    matrix_E = matrix_E.at[2:7, 2:7].set(c_E)\n",
    "    vec_E = np.ravel(matrix_E)\n",
    "    \n",
    "    matrix_I = np.zeros((9,9))\n",
    "    matrix_I = matrix_I.at[2:7, 2:7].set(c_I)\n",
    "    vec_I = np.ravel(matrix_I)\n",
    "    \n",
    "    constant_vec = np.hstack((vec_E, vec_E, vec_I, vec_I))\n",
    "    return constant_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b12438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(x))\n",
    "\n",
    "def binary_loss(n, x):\n",
    "    return - (n*np.log(x) + (1-n)*np.log(1-x))\n",
    "\n",
    "def model(opt_pars, ssn_pars, grid_pars, conn_pars, train_data, filter_pars,  conv_pars):\n",
    "    \n",
    "    signs=np.array([[1, -1], [1, -1]])\n",
    "    \n",
    "    J_2x2 =np.exp(opt_pars['logJ_2x2'])*signs\n",
    "    s_2x2 = np.exp(opt_pars['logs_2x2'])\n",
    "    \n",
    "    #Initialise network\n",
    "    ssn=SSN2DTopoV1_AMPAGABA_ONOFF(ssn_pars=ssn_pars, grid_pars=grid_pars, conn_pars=conn_pars, filter_pars=filter_pars, J_2x2=J_2x2, s_2x2=s_2x2)\n",
    "    \n",
    "    #Create vector using extrasynaptic constants\n",
    "    constant_vector = constant_to_vec(opt_pars['c_E'], opt_pars['c_I'])\n",
    "    \n",
    "    #Apply Gabor filters to stimuli\n",
    "    output_ref=np.matmul(ssn.gabor_filters, train_data['ref'])*ssn.A + constant_vector\n",
    "    output_target=np.matmul(ssn.gabor_filters, train_data['target'])*ssn.A + constant_vector\n",
    "    \n",
    "\n",
    "    #Rectify output\n",
    "    SSN_input_ref=np.maximum(0, output_ref)\n",
    "    SSN_input_target=np.maximum(0, output_target)\n",
    "\n",
    "    #Input to SSN\n",
    "    r_init = np.zeros(SSN_input_ref.shape[0])\n",
    "\n",
    "    fp_ref, _ = ssn.fixed_point_r(SSN_input_ref, r_init=r_init, **conv_pars)\n",
    "    x_ref = ssn.apply_bounding_box(fp_ref, size=3.2)\n",
    "\n",
    "    fp_target, _ = ssn.fixed_point_r(SSN_input_target, r_init=r_init, **conv_pars)\n",
    "    x_target = ssn.apply_bounding_box(fp_target, size=3.2)\n",
    "\n",
    "    #Apply sigmoid function - combine ref and target\n",
    "    x = sigmoid( np.dot(opt_pars['w_sig'], (x_ref.ravel() - x_target.ravel())) + opt_pars['b_sig'])\n",
    "\n",
    "    #Calculate binary cross entropy loss\n",
    "    loss=binary_loss(train_data['label'], x)\n",
    "   \n",
    "    return loss\n",
    "\n",
    "\n",
    "def loss(opt_pars, ssn_pars, grid_pars, conn_pars, train_data, filter_pars,  conv_pars):\n",
    "    '''\n",
    "    Calculate parallelized loss for batch of data through vmap.\n",
    "    Output:\n",
    "        mean loss of all the input images\n",
    "    '''\n",
    "    \n",
    "    vmap_model = vmap(model, in_axes = ({'b_sig': None, 'c_E':None, 'c_I': None, 'logJ_2x2': None, 'logs_2x2': None, 'w_sig': None}, None, None, {'PERIODIC': None, 'p_local': [None, None], 'sigma_oris': None},  {'ref':0, 'target':0, 'label':0}, {'conv_factor': None, 'degree_per_pixel': None, 'edge_deg': None, 'k': None, 'sigma_g': None}, {'Tmax': None, 'dt': None, 'silent': None, 'verbose': None, 'xtol': None}) )                   \n",
    "    loss = np.sum(vmap_model(opt_pars, ssn_pars, grid_pars, conn_pars, train_data, filter_pars,  conv_pars))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def eval_model(opt_pars, ssn_pars, grid_pars, conn_pars, test_data, filter_pars,  conv_pars):\n",
    "    signs=np.array([[1, -1], [1, -1]])\n",
    "    \n",
    "    J_2x2 =np.exp(opt_pars['logJ_2x2'])*signs\n",
    "    s_2x2 = np.exp(opt_pars['logs_2x2'])\n",
    "    \n",
    "    #Initialise network\n",
    "    ssn=SSN2DTopoV1_AMPAGABA_ONOFF(ssn_pars=ssn_pars, grid_pars=grid_pars, conn_pars=conn_pars, filter_pars=filter_pars, J_2x2=J_2x2, s_2x2=s_2x2)\n",
    "\n",
    "    #Apply Gabor filters to stimuli\n",
    "    output_ref=np.matmul(ssn.gabor_filters, test_data['ref'])*ssn.A\n",
    "    output_target=np.matmul(ssn.gabor_filters, test_data['target'])*ssn.A\n",
    "\n",
    "    #Rectify output\n",
    "    SSN_input_ref=np.maximum(0, output_ref)\n",
    "    SSN_input_target=np.maximum(0, output_target)\n",
    "\n",
    "    #Input to SSN\n",
    "    r_init = np.zeros(SSN_input_ref.shape[0])\n",
    "\n",
    "    fp_ref, _ = ssn.fixed_point_r(SSN_input_ref, r_init=r_init, **conv_pars)\n",
    "    x_ref = ssn.apply_bounding_box(fp_ref, size=3.2)\n",
    "\n",
    "    fp_target, _ = ssn.fixed_point_r(SSN_input_target, r_init=r_init, **conv_pars)\n",
    "    x_target = ssn.apply_bounding_box(fp_target, size=3.2)\n",
    "    \n",
    "    dot= np.dot(opt_pars['w_sig'], (x_ref.ravel() - x_target.ravel()))\n",
    "\n",
    "    #Apply sigmoid function - combine ref and target\n",
    "    x = sigmoid( np.dot(opt_pars['w_sig'], (x_ref.ravel() - x_target.ravel())) + opt_pars['b_sig'])\n",
    "    \n",
    "    #compare prediction to label\n",
    "    pred_label = np.round(x)\n",
    "\n",
    "    #Calculate binary cross entropy loss\n",
    "    loss=binary_loss(test_data['label'], x)\n",
    "    \n",
    "    return loss, pred_label, x\n",
    "\n",
    "\n",
    "def vmap_eval(opt_pars, ssn_pars, grid_pars, conn_pars, test_data, filter_pars,  conv_pars):\n",
    "    \n",
    "    eval_vmap = vmap(eval_model, in_axes = ({'b_sig': None,  'c_E':None, 'c_I': None,  'logJ_2x2': None, 'logs_2x2': None, 'w_sig': None}, None, None, {'PERIODIC': None, 'p_local': [None, None], 'sigma_oris': None},  {'ref':0, 'target':0, 'label':0}, {'conv_factor': None, 'degree_per_pixel': None, 'edge_deg': None, 'k': None, 'sigma_g': None}, {'Tmax': None, 'dt': None, 'silent': None, 'verbose': None, 'xtol': None}) )\n",
    "    losses, pred_labels, dots = eval_vmap(opt_pars, ssn_pars, grid_pars, conn_pars, test_data, filter_pars,  conv_pars)\n",
    "    \n",
    "    accuracy = np.sum(test_data['label'] == pred_labels)/len(test_data['label']) \n",
    "    \n",
    "    vmap_loss= np.mean(losses)\n",
    "    \n",
    "    return vmap_loss, accuracy, dots\n",
    "\n",
    "\n",
    "def train_SSN_vmap(opt_pars, ssn_pars, grid_pars, conn_pars, stimuli_pars, filter_pars, conv_pars, epochs_to_save, batch_size=20, ref_ori = 55, offset = 5, epochs=1, eta=10e-4):\n",
    "    \n",
    "    #Initialize loss\n",
    "    val_loss_per_epoch = []\n",
    "    training_losses=[]\n",
    "    accuracies=[]\n",
    "    \n",
    "    #Saving parameters\n",
    "    all_J = numpy.zeros((4, len(epochs_to_save)))\n",
    "    all_s = numpy.zeros((4, len(epochs_to_save)))\n",
    "    all_c = numpy.zeros((2, len(epochs_to_save)))\n",
    "    count = 0\n",
    "        \n",
    "    #Initialise optimizer\n",
    "    optimizer = optax.adam(eta)\n",
    "    opt_state = optimizer.init(opt_pars)\n",
    "    \n",
    "    #Define test data - no need to iterate\n",
    "    test_data = create_data(stimuli_pars, number = batch_size, offset = offset, ref_ori = ref_ori)\n",
    "\n",
    "    val_loss, accuracy, _= vmap_eval(opt_pars, ssn_pars, grid_pars, conn_pars, test_data, filter_pars,  conv_pars)\n",
    "    \n",
    "    print('Training model at ref orientation {}, offset {},  number of epochs {}'.format(ref_ori, offset, epochs))\n",
    "\n",
    "    print('Before training  -- loss: {}, accuracy: {} '.format(val_loss, accuracy))\n",
    "    val_loss_per_epoch.append(val_loss)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    all_opt_pars = dict(logJ_2x2 = [], logs_2x2 = [], w_sig = [], b_sig=[])\n",
    "    \n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0 \n",
    "           \n",
    "\n",
    "        #Create data\n",
    "        train_data = create_data(stimuli_pars, number = batch_size, offset = offset, ref_ori = ref_ori)\n",
    "\n",
    "        #Compute loss and gradient\n",
    "        batch_loss, grad =jax.value_and_grad(loss)(opt_pars, ssn_pars, grid_pars, conn_pars, train_data, filter_pars,  conv_pars)\n",
    "\n",
    "        #Apply SGD through Adam optimizer per batch\n",
    "        updates, opt_state = optimizer.update(grad, opt_state)\n",
    "        opt_pars = optax.apply_updates(opt_pars, updates)\n",
    "        epoch_loss+=batch_loss\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        #Save params and evaluate model \n",
    "        if epoch in epochs_to_save:\n",
    "            all_J, all_s, all_c, count = save_params(all_J, all_s, all_c, opt_pars, count=count)\n",
    "            \n",
    "            #Evaluate model at the end of each epoch\n",
    "            test_data = create_data(stimuli_pars, number = batch_size, offset = offset, ref_ori = ref_ori)\n",
    "            val_loss, accuracy, _= vmap_eval(opt_pars, ssn_pars, grid_pars, conn_pars, test_data, filter_pars,  conv_pars)\n",
    "            print('Training loss: {} ¦ Validation -- loss: {}, accuracy: {} at epoch {}, (time {})'.format(epoch_loss, val_loss, accuracy, epoch, epoch_time))\n",
    "            val_loss_per_epoch.append(val_loss)\n",
    "            training_losses.append(epoch_loss)\n",
    "            accuracies.append(accuracy)\n",
    "         \n",
    "    \n",
    "    \n",
    "    #reparametize parameters\n",
    "    signs=np.array([[1, -1], [1, -1]])    \n",
    "    opt_pars['logJ_2x2'] = np.exp(opt_pars['logJ_2x2'])*signs\n",
    "    opt_pars['logs_2x2'] = np.exp(opt_pars['logs_2x2'])\n",
    "    \n",
    "    return opt_pars, all_J, all_s, all_c, val_loss_per_epoch, training_losses, accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9826b188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEW NOISE!!\n",
    "epochs = 1000\n",
    "#epochs_to_save = np.linspace(1 ,epochs, 11).astype(int)\n",
    "epochs_to_save = np.unique(np.logspace(0, 3, 200).astype(int))\n",
    "vmap_pars_10, all_J, all_s, all_c, vmap_val_loss_10, train_loss_10, acc_10 = train_SSN_vmap(opt_pars, ssn_pars, grid_pars, conn_pars, stimuli_pars, filter_pars,  conv_pars, epochs_to_save = epochs_to_save, ref_ori = 55, offset = 2, batch_size = 50, epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9087648",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(15,10))\n",
    "\n",
    "ax[0,0].plot(epochs_to_save, all_J.T, label =['J_EE', 'JIE', 'J_EI', 'J_II'])\n",
    "ax[0,0].set_title('J')\n",
    "ax[0,0].legend()\n",
    "\n",
    "ax[0,1].plot(epochs_to_save, all_s.T, label =['s_EE', 's_IE', 's_EI', 's_II'])\n",
    "ax[0,1].set_title('s')\n",
    "ax[0,1].legend()\n",
    "\n",
    "ax[1,0].plot(epochs_to_save, all_c.T, label =['c_E', 'c_I'])\n",
    "ax[1,0].set_title('Constants')\n",
    "ax[1,0].legend()\n",
    "\n",
    "ax[1,1].plot(acc_10)\n",
    "ax[1,1].set_title('Accuracy')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566628b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
