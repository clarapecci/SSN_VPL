{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f6c9db",
   "metadata": {},
   "source": [
    "## TRAINING IMPLEMENTATION "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0482e672",
   "metadata": {},
   "source": [
    "In this notebook: \n",
    "- training script to be run on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b778e5e",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a96f7e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax backend cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os, json\n",
    "import pandas as pd\n",
    "from scipy import stats \n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import jax\n",
    "\n",
    "from jax import random\n",
    "from jax.config import config \n",
    "import jax.numpy as np\n",
    "from jax import vmap\n",
    "import pdb\n",
    "import optax\n",
    "from functools import partial\n",
    "import math\n",
    "import csv\n",
    "import time\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy\n",
    "\n",
    "from jax.lib import xla_bridge\n",
    "print(\"jax backend {}\".format(xla_bridge.get_backend().platform))\n",
    "#config.update('jax_debug_nans', True)\n",
    "from SSN_classes_jax_A import SSN2DTopoV1_AMPAGABA_ONOFF\n",
    "from util import GaborFilter, BW_Grating, find_A, create_gabor_filters, create_gratings, param_ratios, plot_results, create_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735f18bf",
   "metadata": {},
   "source": [
    "->Check GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "769bbfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "gpu = jax.devices()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71f005",
   "metadata": {},
   "source": [
    "## 2. PARAMETERS TO DEFINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246fdcd9",
   "metadata": {},
   "source": [
    "### 2.1 Stimuli parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c78a1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gabor parameters \n",
    "sigma_g= 0.5\n",
    "k = np.pi/(6*sigma_g)\n",
    "\n",
    "#Stimuli parameters\n",
    "ref_ori = 55\n",
    "offset = 5\n",
    "\n",
    "#Assemble parameters in dictionary\n",
    "general_pars = dict(k=k , edge_deg=3.2,  degree_per_pixel=0.05)\n",
    "stimuli_pars = dict(outer_radius=3, inner_radius=2.5, grating_contrast=0.8, std = 15, jitter_val = 5)\n",
    "stimuli_pars.update(general_pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def46ec0",
   "metadata": {},
   "source": [
    "### 2.2. Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2864d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network parameters\n",
    "class ssn_pars():\n",
    "    n = 2\n",
    "    k = 0.04\n",
    "    tauE = 30 # in ms\n",
    "    tauI = 10 # in ms~\n",
    "    psi = 0.774\n",
    "    A=None\n",
    "    tau_s = np.array([5, 7, 100]) #in ms, AMPA, GABA, NMDA current decay time constants\n",
    "    \n",
    "\n",
    "#Grid parameters\n",
    "class grid_pars():\n",
    "    gridsize_Nx = 9 # grid-points across each edge # gives rise to dx = 0.8 mm\n",
    "    gridsize_deg = 2 * 1.6 # edge length in degrees\n",
    "    magnif_factor = 2  # mm/deg\n",
    "    hyper_col = 0.8 # mm   \n",
    "    sigma_RF = 0.4 # deg (visual angle)\n",
    "\n",
    "# Caleb's params for the full (with local) model:\n",
    "Js0 = [1.82650658, 0.68194475, 2.06815311, 0.5106321]\n",
    "gE, gI = 0.57328625, 0.26144141\n",
    "\n",
    "sigEE, sigIE = 0.2, 0.40\n",
    "sigEI, sigII = .09, .09\n",
    "conn_pars = dict(\n",
    "    PERIODIC = False,\n",
    "    p_local = [.4, 0.7], # [p_local_EE, p_local_IE],\n",
    "    sigma_oris = 1000) # sigma_oris\n",
    "\n",
    "\n",
    "make_J2x2 = lambda Jee, Jei, Jie, Jii: np.array([[Jee, -Jei], [Jie,  -Jii]]) * np.pi * ssn_pars.psi\n",
    "J_2x2 = make_J2x2(*Js0)\n",
    "s_2x2 = np.array([[sigEE, sigEI],[sigIE, sigII]])\n",
    "\n",
    "#Parameters exclusive to Gabor filters\n",
    "filter_pars = dict(sigma_g = sigma_g, conv_factor = grid_pars.magnif_factor)\n",
    "filter_pars.update(general_pars) \n",
    "\n",
    "\n",
    "#Positive reparameterization\n",
    "signs=np.array([[1, -1], [1, -1]])\n",
    "logJ_2x2 =np.log(J_2x2*signs)\n",
    "logs_2x2 = np.log(s_2x2)\n",
    "\n",
    "#Excitatory and inhibitory constants for extra synaptic GABA\n",
    "c_E = 5.0\n",
    "c_I = 5.0\n",
    "\n",
    "#Sigmoid parameters\n",
    "N_neurons = 25\n",
    "\n",
    "#key, _ = random.split(key)\n",
    "#w_sig = random.normal(key, shape = (N_neurons,)) / np.sqrt(N_neurons)\n",
    "w_sig = numpy.random.normal(size=(N_neurons,)) / np.sqrt(N_neurons)\n",
    "#w_sig = np.zeros((N_neurons))\n",
    "b_sig = 0.0\n",
    "\n",
    "#Optimization pars\n",
    "opt_pars = dict(logJ_2x2 = logJ_2x2, logs_2x2 = logs_2x2, w_sig = w_sig, b_sig=b_sig, c_E = c_E, c_I = c_I)\n",
    "\n",
    "#Convergence parameters\n",
    "conv_pars=dict(dt = 1, xtol = 1e-5, Tmax = 800, verbose=False, silent=True)\n",
    "\n",
    "loss_pars = dict(lambda_1=1, lambda_2=1, lambda_w= 1, lambda_b=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b02e2009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lambda_1': 1, 'lambda_2': 1, 'lambda_w': 1, 'lambda_b': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb569907",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conn_pars():\n",
    "    PERIODIC = False\n",
    "    p_local = [0.4, 0.7]\n",
    "    sigma_oris = 1000\n",
    "    \n",
    "class filter_pars():\n",
    "    sigma_g = numpy.array(0.5)\n",
    "    conv_factor = numpy.array(2)\n",
    "    k = numpy.array(1.0471975511965976)\n",
    "    edge_deg = numpy.array( 3.2)\n",
    "    degree_per_pixel = numpy.array(0.05)\n",
    "    \n",
    "class conv_pars:\n",
    "    dt = 1\n",
    "    xtol = 1e-05\n",
    "    Tmax = 1000\n",
    "    verbose = False\n",
    "    silent = True\n",
    "\n",
    "class loss_pars:\n",
    "    lambda_1 = 1\n",
    "    lambda_2 = 1\n",
    "    lambda_w = 1\n",
    "    lambda_b = 1\n",
    "    \n",
    "ssn=SSN2DTopoV1_AMPAGABA_ONOFF(ssn_pars=ssn_pars, grid_pars=grid_pars, conn_pars=conn_pars, filter_pars=filter_pars, J_2x2=J_2x2, s_2x2=s_2x2)\n",
    "ssn_pars.A=ssn.A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf068b",
   "metadata": {},
   "source": [
    "### 2.3 Training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4693394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name of results csv\n",
    "home_dir = os.getcwd()\n",
    "\n",
    "#Create directory for results\n",
    "results_dir = os.path.join(home_dir, 'results')\n",
    "if os.path.exists(results_dir) == False:\n",
    "        os.makedirs(results_dir)\n",
    "        \n",
    "        \n",
    "results_name = 'testing_cube.csv' #SPECIFY NAME OF RESULTS FILE\n",
    "if results_name == None:\n",
    "    results_name = 'results.csv'\n",
    "\n",
    "results_filename = os.path.join(results_dir, results_name)\n",
    "\n",
    "gpus = jax.devices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2f726a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "             16, 17, 18, 19, 20], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of epochs\n",
    "epochs = 20\n",
    "num_epochs_to_save = 20\n",
    "epochs_to_save = np.linspace(1 ,epochs, num_epochs_to_save).astype(int)\n",
    "epochs_to_save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c92551",
   "metadata": {},
   "source": [
    "## 3. TRAINING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b12438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(training_losses, validation_losses, epochs_to_save):\n",
    "    plt.plot(training_losses.T, label = ['Binary cross entropy', 'Avg_dx', 'R_max', 'w', 'b', 'Training total'] )\n",
    "    plt.plot(epochs_to_save, validation_losses, label='Validation')\n",
    "    plt.legend()\n",
    "    plt.title('Training losses')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def create_data(stimuli_pars, number=100, offset = 5, ref_ori=55):\n",
    "    \n",
    "    '''\n",
    "    Create data for given jitter and noise value for testing (not dataloader)\n",
    "    '''\n",
    "    data = create_gratings(ref_ori=ref_ori, number=number, offset=offset, **stimuli_pars)\n",
    "    train_data = next(iter(DataLoader(data, batch_size=len(data), shuffle=False)))\n",
    "    train_data['ref'] = train_data['ref'].numpy()\n",
    "    train_data['target'] = train_data['target'].numpy()\n",
    "    train_data['label'] = train_data['label'].numpy()\n",
    "    \n",
    "    return train_data\n",
    "\n",
    "\n",
    "def save_params_dict(opt_pars, true_acc, ber_acc, epoch ):\n",
    "    J_2x2, s_2x2 = exponentiate(opt_pars)\n",
    "     \n",
    "    save_params= dict(val_accuracy= true_acc, \n",
    "                      ber_accuracy = ber_acc,\n",
    "                J_EE= J_2x2[0,0], J_EI = J_2x2[0,1], \n",
    "                              J_IE = J_2x2[1,0], J_II = J_2x2[1,1], \n",
    "                s_EE= s_2x2[0,0], s_EI = s_2x2[0,1], \n",
    "                              s_IE = s_2x2[1,0], s_II = s_2x2[1,1],\n",
    "                c_E = opt_pars['c_E'], c_I = opt_pars['c_I'], \n",
    "                 epoch = epoch, w_sig = opt_pars['w_sig'], b_sig=opt_pars['b_sig'])\n",
    "    \n",
    "    return save_params\n",
    "\n",
    "def constant_to_vec(c_E, c_I):\n",
    "    \n",
    "    matrix_E = np.zeros((9,9))\n",
    "    matrix_E = matrix_E.at[2:7, 2:7].set(c_E)\n",
    "    vec_E = np.ravel(matrix_E)\n",
    "    \n",
    "    matrix_I = np.zeros((9,9))\n",
    "    matrix_I = matrix_I.at[2:7, 2:7].set(c_I)\n",
    "    vec_I = np.ravel(matrix_I)\n",
    "    \n",
    "    constant_vec = np.hstack((vec_E, vec_E, vec_I, vec_I))\n",
    "    return constant_vec\n",
    "\n",
    "def sigmoid(x, epsilon = 0.001):\n",
    "    \n",
    "    '''\n",
    "    Introduction of epsilon stops asymptote from reaching 1 (avoids NaN)\n",
    "    '''\n",
    "   \n",
    "    sig = 1/(1+np.exp(x))\n",
    "    \n",
    "    return (1 - 2*epsilon)*sig + epsilon\n",
    "\n",
    "\n",
    "def binary_loss(n, x):\n",
    "    return - (n*np.log(x) + (1-n)*np.log(1-x))\n",
    "\n",
    "def exponentiate(opt_pars):\n",
    "    signs=np.array([[1, -1], [1, -1]]) \n",
    "    \n",
    "    J_2x2 =np.exp(opt_pars['logJ_2x2'])*signs\n",
    "    s_2x2 = np.exp(opt_pars['logs_2x2'])\n",
    "    \n",
    "    return J_2x2, s_2x2\n",
    "\n",
    "def our_max(x, beta=1):\n",
    "    max_val = np.log(np.sum(np.exp(x*beta)))/beta\n",
    "    return max_val\n",
    "\n",
    "\n",
    "def model(opt_pars, ssn_pars, grid_pars, conn_pars, train_data, filter_pars,  conv_pars, loss_pars, bernoulli, sig_noise):\n",
    "    \n",
    "    J_2x2, s_2x2 = exponentiate(opt_pars)\n",
    "    \n",
    "    #Initialise network\n",
    "    ssn=SSN2DTopoV1_AMPAGABA_ONOFF(ssn_pars=ssn_pars, grid_pars=grid_pars, conn_pars=conn_pars, filter_pars=filter_pars, J_2x2=J_2x2, s_2x2=s_2x2)\n",
    "    \n",
    "    #Create vector using extrasynaptic constants\n",
    "    constant_vector = constant_to_vec(opt_pars['c_E'], opt_pars['c_I'])\n",
    "    \n",
    "    #Apply Gabor filters to stimuli\n",
    "    output_ref=np.matmul(ssn.gabor_filters, train_data['ref']) + constant_vector\n",
    "    output_target=np.matmul(ssn.gabor_filters, train_data['target']) + constant_vector\n",
    "    \n",
    "    #Rectify output\n",
    "    SSN_input_ref=np.maximum(0, output_ref)\n",
    "    SSN_input_target=np.maximum(0, output_target)\n",
    "\n",
    "    #Find the fixed point \n",
    "    x_ref, r_max_ref, avg_dx_ref = obtain_fixed_point(ssn, SSN_input_ref, conv_pars)\n",
    "    x_target, r_max_target, avg_dx_target = obtain_fixed_point(ssn, SSN_input_target, conv_pars)\n",
    "    \n",
    "    #Add additional noise before sigmoid layer\n",
    "    if sig_noise:\n",
    "        delta_x = x_ref.ravel() - x_target.ravel() \n",
    "        external_noise = sig_noise*numpy.random.normal(size=((x_target.ravel()).shape))\n",
    "        delta_x = delta_x + external_noise\n",
    "    else:\n",
    "        delta_x = x_ref.ravel() - x_target.ravel() \n",
    "    \n",
    "    #Apply sigmoid function - combine ref and target\n",
    "    x = sigmoid( np.dot(opt_pars['w_sig'], (delta_x)) + opt_pars['b_sig'])\n",
    "\n",
    "    #Calculate losses\n",
    "    loss_binary=binary_loss(train_data['label'], x)\n",
    "    loss_avg_dx = loss_pars.lambda_1*(avg_dx_ref + avg_dx_target)/2\n",
    "    loss_r_max =  loss_pars.lambda_2*(r_max_ref + r_max_target)/2\n",
    "    loss_w = loss_pars.lambda_w*(np.linalg.norm(opt_pars['w_sig'])**2)\n",
    "    loss_b = loss_pars.lambda_b*(opt_pars['b_sig']**2)\n",
    "    \n",
    "    #Combine all losses\n",
    "    loss = loss_binary +  loss_avg_dx + loss_r_max  + loss_w + loss_b\n",
    "    all_losses = np.vstack((loss_binary, loss_avg_dx, loss_r_max, loss_w, loss_b, loss))\n",
    "    \n",
    "    pred_label = np.round(x) \n",
    "    \n",
    "    #Calculate predicted label using Bernoulli distribution\n",
    "    if bernoulli==True:\n",
    "        key_int = numpy.random.randint(low = 0, high =  10000)\n",
    "        key = random.PRNGKey(key_int)\n",
    "        pred_label_b = np.sum(jax.random.bernoulli(key, p=x, shape=None))\n",
    "        pred_label = [pred_label, pred_label_b]\n",
    "    \n",
    "    return loss, all_losses, pred_label\n",
    "\n",
    "\n",
    "def obtain_fixed_point(ssn, ssn_input, conv_pars,  Rmax_E = 50, Rmax_I = 100):\n",
    "    \n",
    "    r_init = np.zeros(ssn_input.shape[0])\n",
    "    \n",
    "    dt = conv_pars.dt\n",
    "    xtol = conv_pars.xtol\n",
    "    Tmax = conv_pars.Tmax\n",
    "    verbose = conv_pars.verbose\n",
    "    silent = conv_pars.silent\n",
    "    \n",
    "    #Find fixed point  \n",
    "   \n",
    "    fp, _, avg_dx = ssn.fixed_point_r(ssn_input, r_init=r_init, dt=dt, xtol=xtol, Tmax=Tmax, verbose = verbose, silent=silent)\n",
    "    avg_dx = np.maximum(0, (avg_dx -1))\n",
    "    \n",
    "    #Apply bounding box to data\n",
    "    x_box = ssn.apply_bounding_box(fp, size=3.2)\n",
    "    r_max = np.maximum(0, (our_max(fp[:ssn.Ne])/Rmax_E - 1)) + np.maximum(0, (our_max(fp[ssn.Ne:-1])/Rmax_I - 1))\n",
    "    \n",
    "    return x_box, r_max, avg_dx\n",
    "\n",
    "@partial(jax.jit, static_argnums=(1, 2, 3, 5, 6 , 7 , 8 , 9 ), device=gpu)\n",
    "def loss(opt_pars, ssn_pars, grid_pars, conn_pars, train_data, filter_pars,  conv_pars, loss_pars, bernoulli, sig_noise):\n",
    "    '''\n",
    "    Calculate parallelized loss for batch of data through vmap.\n",
    "    Output:\n",
    "        mean loss of all the input images\n",
    "    '''\n",
    "    \n",
    "    vmap_model = vmap(model, in_axes = ({'b_sig': None,  'c_E':None, 'c_I': None,  'logJ_2x2': None, 'logs_2x2': None, 'w_sig': None}, None, None, None,  {'ref':0, 'target':0, 'label':0}, None, None, None, None, None) )                   \n",
    "    total_loss, all_losses , _= vmap_model(opt_pars, ssn_pars, grid_pars, conn_pars, train_data, filter_pars,  conv_pars, loss_pars, bernoulli, sig_noise)\n",
    "    loss= np.sum(total_loss)\n",
    "    all_losses = np.mean(all_losses, axis = 0)\n",
    "    \n",
    "    return loss, all_losses\n",
    "\n",
    "\n",
    "def vmap_eval(opt_pars, ssn_pars, grid_pars, conn_pars, test_data, filter_pars,  conv_pars, loss_pars, bernoulli, sig_noise):\n",
    "    \n",
    "    eval_vmap = vmap(model, in_axes = ({'b_sig': None,  'c_E':None, 'c_I': None,  'logJ_2x2': None, 'logs_2x2': None, 'w_sig': None}, None, None, None,  {'ref':0, 'target':0, 'label':0}, None, None, None, None, None) )\n",
    "    losses, _, pred_labels = eval_vmap(opt_pars, ssn_pars, grid_pars, conn_pars, test_data, filter_pars, conv_pars, loss_pars, bernoulli, sig_noise) \n",
    "\n",
    "    \n",
    "    #Find accuracy based on predicted labels\n",
    "    true_accuracy = np.sum(test_data['label'] == pred_labels[0])/len(test_data['label']) \n",
    "    ber_accuracy = np.sum(test_data['label'] == pred_labels[1])/len(test_data['label']) \n",
    "    \n",
    "    vmap_loss= np.mean(losses)\n",
    "    \n",
    "    return vmap_loss, true_accuracy, ber_accuracy\n",
    "\n",
    "def train_SSN_vmap(opt_pars, ssn_pars, grid_pars, conn_pars, stimuli_pars, filter_pars, conv_pars, loss_pars, epochs_to_save, results_filename = None, batch_size=20, ref_ori = 55, offset = 5, epochs=1, eta=10e-4, bernoulli=False, sig_noise = None):\n",
    "    \n",
    "    #Initialize loss\n",
    "    val_loss_per_epoch = []\n",
    "    training_losses=[]\n",
    "    \n",
    "    #Initialise optimizer\n",
    "    optimizer = optax.adam(eta)\n",
    "    opt_state = optimizer.init(opt_pars)\n",
    "    \n",
    "    #Define test data - no need to iterate\n",
    "    test_data = create_data(stimuli_pars, number = batch_size, offset = offset, ref_ori = ref_ori)\n",
    "    val_loss, true_acc, ber_acc= vmap_eval(opt_pars, ssn_pars, grid_pars, conn_pars, test_data, filter_pars,  conv_pars, loss_pars, bernoulli, sig_noise)\n",
    "    print('Before training  -- loss: {}, true accuracy: {} , Bernoulli accuracy: {} (learning rate: {})'.format(np.round(float(val_loss), 3), np.round(true_acc, 3), np.round(ber_acc, 3), eta))\n",
    "    val_loss_per_epoch.append(val_loss)\n",
    "    \n",
    "    #Save initial parameters\n",
    "    save_params = save_params_dict(opt_pars=opt_pars, true_acc=true_acc, ber_acc = ber_acc, epoch=0 )\n",
    "    \n",
    "    #Initialise csv file\n",
    "    if results_filename:\n",
    "        results_handle = open(results_filename, 'w')\n",
    "        results_writer = csv.DictWriter(results_handle, fieldnames=save_params.keys())\n",
    "        results_writer.writeheader()\n",
    "        results_writer.writerow(save_params)\n",
    "        print('Saving results to csv ', results_filename)\n",
    "    else:\n",
    "        print('#### NOT SAVING! ####')\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0 \n",
    "           \n",
    "        #Load next batch of data and convert\n",
    "        train_data = create_data(stimuli_pars, number = batch_size, offset = offset, ref_ori = ref_ori)\n",
    "\n",
    "        #Compute loss and gradient\n",
    "        epoch_loss, grad =jax.value_and_grad(loss, has_aux = True)(opt_pars, ssn_pars, grid_pars, conn_pars, train_data, filter_pars, conv_pars, loss_pars, bernoulli, sig_noise)\n",
    "\n",
    "        #Apply SGD through Adam optimizer per batch\n",
    "        updates, opt_state = optimizer.update(grad, opt_state)\n",
    "        opt_pars = optax.apply_updates(opt_pars, updates)\n",
    "        training_losses.append(epoch_loss[0])\n",
    "    \n",
    "        #Save all losses\n",
    "        if epoch==1:\n",
    "            all_losses = epoch_loss[1]\n",
    "        else:\n",
    "            all_losses = np.hstack((all_losses, epoch_loss[1]))\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        #Save the parameters given a number of epochs\n",
    "        if epoch in epochs_to_save:\n",
    "            \n",
    "            #Evaluate model \n",
    "            test_data = create_data(stimuli_pars, number = batch_size, offset = offset, ref_ori = ref_ori)\n",
    "            val_loss, true_acc, ber_acc= vmap_eval(opt_pars, ssn_pars, grid_pars, conn_pars, test_data, filter_pars,  conv_pars, loss_pars, bernoulli, sig_noise)\n",
    "            print('Training loss: {} ¦ Validation -- loss: {}, true accuracy: {}, Bernoulli accuracy: {} at epoch {}, (time {})'.format(epoch_loss[0], val_loss, true_acc, ber_acc, epoch, epoch_time))\n",
    "            val_loss_per_epoch.append(val_loss)\n",
    "            #Create dictionary of parameters to save\n",
    "            save_params = save_params_dict(opt_pars, true_acc, ber_acc, epoch)\n",
    "            \n",
    "            #Write results in csv file\n",
    "            if results_filename:\n",
    "                results_writer.writerow(save_params)\n",
    "\n",
    "    #Reparametize parameters\n",
    "    signs=np.array([[1, -1], [1, -1]])    \n",
    "    opt_pars['logJ_2x2'] = np.exp(opt_pars['logJ_2x2'])*signs\n",
    "    opt_pars['logs_2x2'] = np.exp(opt_pars['logs_2x2'])\n",
    "    \n",
    "   \n",
    "    return opt_pars, val_loss_per_epoch, all_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9826b188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training  -- loss: 2.575000047683716, true accuracy: 0.20000000298023224 , Bernoulli accuracy: 0.320000022649765 (learning rate: 0.001)\n",
      "#### NOT SAVING! ####\n",
      "Training loss: 128.86962890625 ¦ Validation -- loss: 3.24347186088562, true accuracy: 0.41999998688697815, Bernoulli accuracy: 0.41999998688697815 at epoch 1, (time 25.551830530166626)\n",
      "Training loss: 123.69664001464844 ¦ Validation -- loss: 2.3304288387298584, true accuracy: 0.20000000298023224, Bernoulli accuracy: 0.46000000834465027 at epoch 2, (time 25.7451069355011)\n",
      "Training loss: 132.1101837158203 ¦ Validation -- loss: 2.270595073699951, true accuracy: 0.18000000715255737, Bernoulli accuracy: 0.20000000298023224 at epoch 3, (time 3.1810145378112793)\n",
      "Training loss: 136.01119995117188 ¦ Validation -- loss: 2.9321980476379395, true accuracy: 0.47999998927116394, Bernoulli accuracy: 0.47999998927116394 at epoch 4, (time 3.092853307723999)\n",
      "Training loss: 123.32292175292969 ¦ Validation -- loss: 2.506542444229126, true accuracy: 0.20000000298023224, Bernoulli accuracy: 0.23999999463558197 at epoch 5, (time 3.183263063430786)\n",
      "Training loss: 126.57128143310547 ¦ Validation -- loss: 3.6344916820526123, true accuracy: 0.5600000023841858, Bernoulli accuracy: 0.5600000023841858 at epoch 6, (time 3.3631396293640137)\n",
      "Training loss: 126.82779693603516 ¦ Validation -- loss: 2.6289143562316895, true accuracy: 0.4399999976158142, Bernoulli accuracy: 0.46000000834465027 at epoch 7, (time 3.2294700145721436)\n",
      "Training loss: 132.9035186767578 ¦ Validation -- loss: 2.153528928756714, true accuracy: 0.20000000298023224, Bernoulli accuracy: 0.5 at epoch 8, (time 3.2644217014312744)\n",
      "Training loss: 120.1739730834961 ¦ Validation -- loss: 2.4028635025024414, true accuracy: 0.3799999952316284, Bernoulli accuracy: 0.18000000715255737 at epoch 9, (time 3.3340137004852295)\n",
      "Training loss: 118.4398193359375 ¦ Validation -- loss: 2.162320852279663, true accuracy: 0.20000000298023224, Bernoulli accuracy: 0.3400000035762787 at epoch 10, (time 3.1546850204467773)\n",
      "Training loss: 125.47697448730469 ¦ Validation -- loss: 2.3750479221343994, true accuracy: 0.3199999928474426, Bernoulli accuracy: 0.30000001192092896 at epoch 11, (time 3.219627857208252)\n",
      "Training loss: 110.2669448852539 ¦ Validation -- loss: 3.1393465995788574, true accuracy: 0.5600000023841858, Bernoulli accuracy: 0.20000000298023224 at epoch 12, (time 3.1402575969696045)\n",
      "Training loss: 121.70683288574219 ¦ Validation -- loss: 2.01786208152771, true accuracy: 0.4000000059604645, Bernoulli accuracy: 0.30000001192092896 at epoch 13, (time 3.2885005474090576)\n",
      "Training loss: 117.74154663085938 ¦ Validation -- loss: 2.2659389972686768, true accuracy: 0.5799999833106995, Bernoulli accuracy: 0.5 at epoch 14, (time 3.1856768131256104)\n",
      "Training loss: 120.25080871582031 ¦ Validation -- loss: 2.8431994915008545, true accuracy: 0.4399999976158142, Bernoulli accuracy: 0.4399999976158142 at epoch 15, (time 3.1493284702301025)\n",
      "Training loss: 110.54319763183594 ¦ Validation -- loss: 2.2091004848480225, true accuracy: 0.11999999731779099, Bernoulli accuracy: 0.11999999731779099 at epoch 16, (time 3.2766993045806885)\n",
      "Training loss: 113.70634460449219 ¦ Validation -- loss: 1.9493426084518433, true accuracy: 0.4000000059604645, Bernoulli accuracy: 0.36000001430511475 at epoch 17, (time 3.210191488265991)\n",
      "Training loss: 113.1778793334961 ¦ Validation -- loss: 2.7864174842834473, true accuracy: 0.41999998688697815, Bernoulli accuracy: 0.41999998688697815 at epoch 18, (time 3.1104702949523926)\n",
      "Training loss: 111.0503158569336 ¦ Validation -- loss: 2.01935076713562, true accuracy: 0.3199999928474426, Bernoulli accuracy: 0.46000000834465027 at epoch 19, (time 3.176077127456665)\n",
      "Training loss: 121.6346435546875 ¦ Validation -- loss: 2.5614593029022217, true accuracy: 0.5, Bernoulli accuracy: 0.3799999952316284 at epoch 20, (time 3.4714791774749756)\n"
     ]
    }
   ],
   "source": [
    "new_pars, val_loss_per_epoch, training_losses = train_SSN_vmap(opt_pars, ssn_pars, grid_pars, conn_pars, stimuli_pars, filter_pars,  conv_pars, loss_pars, epochs_to_save = epochs_to_save, results_filename=None, ref_ori = 55, offset = 10, batch_size = 50, epochs = epochs, bernoulli = True, sig_noise =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d05e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb.pm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a412899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_to_save = np.insert(epochs_to_save, 0, 0)\n",
    "plot_losses(training_losses, val_loss_per_epoch, epochs_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154c571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from PIL import Image\n",
    "_BLACK = 0\n",
    "_WHITE = 255\n",
    "_GRAY = round((_WHITE + _BLACK) / 2)\n",
    "import jax.numpy as numpy\n",
    "\n",
    "class JiaGrating:\n",
    "\n",
    "    def __init__(self, ori_deg, size, outer_radius, inner_radius, pixel_per_degree, grating_contrast, phase, jitter, std = 0, spatial_frequency=None, ):\n",
    "        self.ori_deg = ori_deg\n",
    "        self.size = size\n",
    "\n",
    "        self.outer_radius = outer_radius #in degrees\n",
    "        self.inner_radius = inner_radius #in degrees\n",
    "        self.pixel_per_degree = pixel_per_degree\n",
    "        self.grating_contrast = grating_contrast\n",
    "        self.phase = phase\n",
    "        self.jitter =  jitter\n",
    "        self.std = std\n",
    "\n",
    "        self.smooth_sd = self.pixel_per_degree / 6\n",
    "        self.spatial_freq = spatial_frequency or (1 / self.pixel_per_degree)\n",
    "        self.grating_size = round(self.outer_radius * self.pixel_per_degree)\n",
    "        self.angle = ((self.ori_deg + self.jitter) - 90) / 180 * numpy.pi\n",
    "\n",
    "    def image(self):\n",
    "        x, y = numpy.mgrid[-self.grating_size:self.grating_size+1., -self.grating_size:self.grating_size+1.]\n",
    "\n",
    "        d = self.grating_size * 2 + 1\n",
    "        annulus = numpy.ones((d, d))\n",
    "\n",
    "        edge_control = numpy.divide(numpy.sqrt(numpy.power(x, 2) + numpy.power(y, 2)), self.pixel_per_degree)\n",
    "\n",
    "        overrado = numpy.nonzero(edge_control > self.inner_radius)\n",
    "\n",
    "        for idx_x, idx_y in zip(*overrado):\n",
    "            #annulus[idx_x, idx_y] = annulus[idx_x, idx_y] * numpy.exp(-1 * ((((edge_control[idx_x, idx_y] - self.inner_radius) * self.pixel_per_degree) ** 2) / (2 * (self.smooth_sd ** 2))))    \n",
    "            annulus.at[idx_x, idx_y].set(annulus[idx_x, idx_y] * numpy.exp(-1 * ((((edge_control[idx_x, idx_y] - self.inner_radius) * self.pixel_per_degree) ** 2) / (2 * (self.smooth_sd ** 2)))))\n",
    "        gabor_sti = _GRAY * (1 + self.grating_contrast * numpy.cos(2 * math.pi * self.spatial_freq * (y * numpy.sin(self.angle) + x * numpy.cos(self.angle)) + self.phase))\n",
    "\n",
    "        gabor_sti[numpy.sqrt(numpy.power(x, 2) + numpy.power(y, 2)) > self.grating_size] = _GRAY\n",
    "        \n",
    "        #New noise - Gaussian white noise\n",
    "        noise = numpy.random.normal(loc=0, scale=self.std, size = (d,d))\n",
    "        noisy_gabor_sti = gabor_sti + noise\n",
    "\n",
    "        # Original Noise\n",
    "        #noise = numpy.floor(numpy.sin(norm.rvs(size=(d, d))) * _GRAY) + _GRAY\n",
    "        \n",
    "        #noise_mask = binomial(1, 1-self.snr, size=(d, d)).astype(int)\n",
    "        \n",
    "       #masked_noise = noise * noise_mask\n",
    "\n",
    "        #signal_mask = 1 - noise_mask\n",
    "        #masked_gabor_sti = signal_mask * gabor_sti\n",
    "\n",
    "        #noisy_gabor_sti = masked_gabor_sti + masked_noise\n",
    "        # End noise\n",
    "\n",
    "        gabor_sti_final = numpy.repeat(noisy_gabor_sti[:, :, numpy.newaxis], 3, axis=-1)\n",
    "        alpha_channel = annulus * _WHITE\n",
    "        gabor_sti_final_with_alpha = numpy.concatenate((gabor_sti_final, alpha_channel[:, :, numpy.newaxis]), axis=-1)\n",
    "        gabor_sti_final_with_alpha_image = Image.fromarray(gabor_sti_final_with_alpha.astype(numpy.uint8))\n",
    "\n",
    "        center_x = int(self.size / 2)\n",
    "        center_y = int(self.size / 2)\n",
    "        bounding_box = (center_x - self.grating_size, center_y - self.grating_size)\n",
    "\n",
    "        background = numpy.full((self.size, self.size, 3), _GRAY, dtype=numpy.uint8)\n",
    "        final_image = Image.fromarray(background)\n",
    "\n",
    "        final_image.paste(gabor_sti_final_with_alpha_image, box=bounding_box, mask=gabor_sti_final_with_alpha_image)\n",
    "        #print(numpy.mean(noisy_gabor_sti) / numpy.std(noisy_gabor_sti))\n",
    "\n",
    "        return final_image\n",
    "\n",
    "\n",
    "class BW_Grating(JiaGrating):\n",
    "\n",
    "    \n",
    "    def __init__(self, ori_deg, outer_radius, inner_radius, degree_per_pixel, grating_contrast, edge_deg, phase=0, jitter=0, std = 0, k=None, crop_f=None):\n",
    "        \n",
    "        self.crop_f=crop_f\n",
    "        pixel_per_degree=1/degree_per_pixel\n",
    "        size=int(edge_deg*2 *pixel_per_degree) + 1\n",
    "        spatial_frequency = k*degree_per_pixel\n",
    "        \n",
    "        \n",
    "                \n",
    "        super().__init__( ori_deg, size, outer_radius, inner_radius, pixel_per_degree, grating_contrast, phase, jitter, std, spatial_frequency)\n",
    "        \n",
    "    def BW_image(self):\n",
    "        \n",
    "        #generate image using Jia Grating function\n",
    "        original=numpy.array(self.image(), dtype=numpy.float16)\n",
    "        \n",
    "        #sum image over channels\n",
    "        image=numpy.sum(original, axis=2) \n",
    "        \n",
    "        \n",
    "        #crop image\n",
    "        if self.crop_f:\n",
    "            image=image[self.crop_f:-self.crop_f, self.crop_f:-self.crop_f]            \n",
    "        return image \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8826e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref = BW_Grating(ori_deg = 55, jitter=5, **stimuli_pars).BW_image().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317ea7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(ref.reshape(129,129))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c232bfca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
